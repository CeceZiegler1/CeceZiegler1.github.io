{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3ed98647-4d20-4809-a19a-b2a7677d67bf",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learning From Timnit Gebru\n",
    "author: Cece Ziegler\n",
    "date: '2023-04-6'\n",
    "description: \"This blog post illustrates my implementation of singular value decomposition of an image using unsupervised learning.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3104cf1-150a-49af-bf9c-08ea509b2a90",
   "metadata": {},
   "source": [
    "# Learning From Timnit Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1e7b0-a90c-4e9a-9665-51a39a2bedb2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Timnit Gebru grew up in Ethipoia and moved to the United States when she was 15 to flee the Eritrean-Ethiopian war. After experiencing difficulties with being allowed to live in the United States, Gebru finally settled in Sommerville Massachusetts where she attended high school. Gebru is an incrediblly influential and important figure due to her work regarding algorithmic bias in machine learning algorithms, specifically within computer vision. Through many experiences of racial bias Gebru has faced personally, she is passionate and knowelgeable on the topic. After working for world renowned tech companies including Apple and Google, Gebru has expressed the mistreatment and bias she has seen within these big tech companies. African Americans are mis-identified by computer vision technologies at a disproportionate rate. After writing a research paper for Google that detailed the potential negative effects of a new AI technology, a paper she was asked to write, Gebru left google as higher up athorities were not happy with what she wrote and didn't want the paper published. After the poor experience with Google, Gebru doesn't have much intrest in returning to work for a big tech company. Instead, she has started her own research intiative, Black in AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d033fb-aed5-4ccd-aed5-526908d695bc",
   "metadata": {},
   "source": [
    "# Dr. Gebru's Talk\n",
    "In the talk from Dr. Gebru, she addresses issues of fairness and ethics in technology, specifically computer vision. One of the main points Dr. Gebru brings up is how technology has done harm to black people and how black people, especially black women are under represented in the technology field. Although she mentions how technology as a whole has done harm to the black community, she speficially focusses on computer vision. Computer vision is a dangerous technology and we don't know what it is being used for. Gebru brings up many applications that use computer vision technology in unethical way. For example, one app, Faception, profilies people based on their facial image. Other companies use facial deetction softwares during interviews in an attempt to track candidates non verbal responses and judge their performance in the interview based on what the facial dectection software finds. Not only are many facial recognition technologies unethical, but margonalized people are critisized and deprived disproportionatley. Gebru mentioned a few studies that showed that displayed this behavior. First she mentioned how police in Baltimore are unethically obtaining photographs of Black protestors and using those images to arrest them for unwarranted reasons. Next she describes how black women specifically are disproportionatley mis-identified by facial recognition technologies from major tech companies such as Google and Microsoft. Through these examples, Gebru identifies how it is not just the algorithms themselves that are bias, but there is bias in the training data. It is not an AI specific problem, but a problem of who is seen and who is heard. The bottom line is the majority of data collected is on white males. She describes how in an attempt to obtain more diverse datasets, companies unethically obtain pictures and images of people. It is difficult to obtain this data in an ethical way which is where a big issue comes into play with AI and machine learning algorthims being biased due to non-diverse datasets.\n",
    "\n",
    "td;lr: Computer vision and other AI technologies have bias due to a lack of diversity in training data, and they unfairly target people who are the most margonalized and vulnerable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7ffbd-fbe7-4d2c-a310-232066414b0a",
   "metadata": {},
   "source": [
    "# Question\n",
    "What do you think is an effective and tangible step companies can take to address issues of inequality and bias in technology and machine learning programs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
