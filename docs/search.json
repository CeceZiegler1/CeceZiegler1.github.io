[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451 Final Project: Create a machine learning model that predicts a players bat speed utilizing Recursive Feature Elimination, Linear Regression and Random Forest Regression models.\n\n\n\n\n\n\nMay 14, 2023\n\n\nCece Ziegler, David Byrne, Julia Fairbank, Sam Ehrsam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the kernel logistic regression.\n\n\n\n\n\n\nMay 7, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of singular value decomposition of an image using unsupervised learning.\n\n\n\n\n\n\nApr 16, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of singular value decomposition of an image using unsupervised learning.\n\n\n\n\n\n\nApr 6, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of singular value decomposition of an image using unsupervised learning.\n\n\n\n\n\n\nApr 6, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the linear regression, using the analytical formula and gradient decent.\n\n\n\n\n\n\nMar 12, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the linear regression, using the analytical formula and gradient decent.\n\n\n\n\n\n\nMar 12, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of penguin classification.\n\n\n\n\n\n\nMar 7, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the logistic regression, using gradient decent.\n\n\n\n\n\n\nMar 2, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the perceptron algorithm, along with test cases to show its performance.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html",
    "href": "posts/perceptron_notebook/PerceptronBlog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Here is a link to my source code.\nhttps://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/perceptron_notebook/perceptron.py"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#graph-of-change-in-accuracy-with-each-iteration",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#graph-of-change-in-accuracy-with-each-iteration",
    "title": "Perceptron Blog",
    "section": "Graph of Change in Accuracy with each Iteration",
    "text": "Graph of Change in Accuracy with each Iteration\nThis graph shows the change in accuracy of the algorithm with each iteration. We can see here that it didn’t take the algorithm very long to converge to zero and reach an accuracy of 100%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "title": "Perceptron Blog",
    "section": "The graph below shows the accuracy over time of non linearly seperable data",
    "text": "The graph below shows the accuracy over time of non linearly seperable data\nWe can see from this graph that the accuracy fluctuates a lot with each iteration as the algorithm attempts to accuratley sort out the non-linearly separable data. It is unable to reach an accuracy of 100% with the best accuracy it reached coming at around 60%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "title": "Perceptron Blog",
    "section": "Below is a graph of the change in accuracy over time for data with more than 5 features",
    "text": "Below is a graph of the change in accuracy over time for data with more than 5 features\nWe can see from this graph, similar to our 2D linearly separable data, the algorithm did not take a long time to converge to zero and reach an accuracy of 100%\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "CeceZieglerRepo/posts/example-blog-post/index.html",
    "href": "CeceZieglerRepo/posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "CeceZieglerRepo/posts/example-blog-post/index.html#math",
    "href": "CeceZieglerRepo/posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "CeceZieglerRepo/index.html",
    "href": "CeceZieglerRepo/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CeceZieglerRepo/about.html",
    "href": "CeceZieglerRepo/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "CeceZiegler1.github.io/posts/example-blog-post/index.html",
    "href": "CeceZiegler1.github.io/posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "CeceZiegler1.github.io/posts/example-blog-post/index.html#math",
    "href": "CeceZiegler1.github.io/posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "CeceZiegler1.github.io/index.html",
    "href": "CeceZiegler1.github.io/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CeceZiegler1.github.io/about.html",
    "href": "CeceZiegler1.github.io/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html",
    "href": "posts/perceptron_notebook/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Here is a link to my source code.\nhttps://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/perceptron_notebook/perceptron.py"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#graph-of-change-in-accuracy-with-each-iteration",
    "href": "posts/perceptron_notebook/index.html#graph-of-change-in-accuracy-with-each-iteration",
    "title": "Perceptron Blog",
    "section": "Graph of Change in Accuracy with each Iteration",
    "text": "Graph of Change in Accuracy with each Iteration\nThis graph shows the change in accuracy of the algorithm with each iteration. We can see here that it didn’t take the algorithm very long to converge to zero and reach an accuracy of 100%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "href": "posts/perceptron_notebook/index.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "title": "Perceptron Blog",
    "section": "The graph below shows the accuracy over time of non linearly seperable data",
    "text": "The graph below shows the accuracy over time of non linearly seperable data\nWe can see from this graph that the accuracy fluctuates a lot with each iteration as the algorithm attempts to accuratley sort out the non-linearly separable data. It is unable to reach an accuracy of 100% with the best accuracy it reached coming at around 60%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "href": "posts/perceptron_notebook/index.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "title": "Perceptron Blog",
    "section": "Below is a graph of the change in accuracy over time for data with more than 5 features",
    "text": "Below is a graph of the change in accuracy over time for data with more than 5 features\nWe can see from this graph, similar to our 2D linearly separable data, the algorithm did not take a long time to converge to zero and reach an accuracy of 100%\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\n#np.random.seed(12345)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n\ny = 2*y -1\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n0.0"
  },
  {
    "objectID": "posts/gradient_blog/LogRegBlog.html",
    "href": "posts/gradient_blog/LogRegBlog.html",
    "title": "Gradient Decent",
    "section": "",
    "text": "Gradient Decent Implementation\nBelow, I show an example of my implementation of logistic regression using gradient decent. I make data and create a plot to show where my line separated the data. I then print out the loss and the accuracy to show how well the model performed. On average, my model averaged a loss between 0.15-0.19 which makes sense as the data is not linearly seperable, which makes this a reasonable loss.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nPrinting out Gradient Decent Loss\n\n#Loss\nprint(LR.loss)\n\n0.15683836141209298\n\n\n\n\n\nStochastic Gradient Implementation\nBelow is an example of a test of my stochastic gradient implementation. The stochastic gradient is similar to regular gradient decent, however it uses batch sizes to partiton the data and computes the gradient of the batch instead of the whole gradient at once. As we can see from the example below, depending on factors such as the size of the batch, stochastic gradient can help us minimize the loss. After running the regular gradient decent and the stochastic gradient decent many times each, the stochastic gradient decent averages a loss of about .13-.16 which is slightly lower than the loss of regular graident decent. Along with having on average a slightly better loss, the stochastic gradient method converges to a minimum faster than regular gradient decent\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, momentum = False, batch_size = 10)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LRS.loss)\n\n0.1364397783931314\n\n\n\n\nMomentum with Stochastic Gradient\nBelow is an example of a test using my stochastic gradient implementation including momentum. The model is the exact same as the stochastic gradient model, however one parameter is changed. When calling the fit method for regular stochastic gradient, I set the momentum parameter = to false. This indicated to set the coeficent beta to 0, meaning momentum won’t be added in when the weight vector is updated. However, when the momentum parameter is true, this indicates to include momentum, thus setting the beta coefficient to 0.8. Adding in momentum accelerates the process in which we are making our way down the gradient toward the minimum, thus helping to converge to the minimum faster. In this example, we also print out the loss which in this specific example was a solid loss at only 0.13, but the loss tends to remain in the same range as regular stochastic gradient\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, momentum = True, batch_size = 10)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LRS.loss)\n\n0.1364397783931314\n\n\n\n\nLoss History Over Time\nBelow is a graph of each of the three models described above showing their loss history over each epoch/iteration. As we can see from the graph, regular gradient decent takes much longer to converge than stochastic gradient and stochastic gradient with momentum. Both the stochastic with momentum took 100 iteration, and stochastic without momentum took 200 iterations to converge whereas the regular gradient decent took 2100 iterations. This is a very significant difference in time and displays how stochastic gradient implementations can converge faster. We are also able to see from this how momentum helps speed up the time of convergence, as the model that included momentum took 100 less iteration to converge.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n100\n200\n2100\n\n\n\n\n\n\n\nAlpha too big to Converge\nBelow is an example in which the size of the alpha is too large which causes the model to be unable to converge to the minimum. In this test example, I set the alpha to 35 and the max_epochs to 100. Although the model still completed running, it’s loss was 0.26 which is much higher than we want and higher than any of the above model examples. This shows that the alpha size was too large, and the model was unable to converge to the proper minimum.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 30, max_epochs = 100)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR.loss\n\n0.2608236468870702\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 2, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient small batch\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 20, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient large batch\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n#This shows that small batch size takes longer to converge, 100 iterations for large batch compared to 600 for small batch\n\n100\n300\n\n\n\n\n\n\n\nExample showing batch size influencing convergence speed.\nIn the graph above, we can see how batch size affects the speed of convergence. Here, we show the convergence of two regular stochastic gradients, one with a batch size of 2, and the other with a batch size of 20. The function with a batch size of 20 took 100 iterations to converge, whereas the function with a batch size of 2 took 300 iterations to converge. This is three times the amount of iterations which shows that having a small batch size takes a longer time to converge.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 500, \n                  momentum = True, \n                  batch_size = 250, \n                  alpha = 0.1) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 500, \n                  momentum = False, \n                  batch_size = 250, \n                  alpha = 0.1)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n#Momentum converges faster when the batch size is large\n\n500\n1000\n\n\n\n\n\n\n\nExample: Momentum Converging Faster than Regular Stochastic:\nThe graph above shows an example of when momentum significantly speeds up convergence. In this example, we have a large batch size. With the large batch size, the stochastic momentum gradient only takes 500 iterations to converge, whereas the regular stochastic gradient takes 1000 iterations to converge. This is double the amount of iterations, showing that when the batch size is large, momentum helps the stochastic gradeient converge faster."
  },
  {
    "objectID": "posts/PenguinClassification/PenguinClassification.html",
    "href": "posts/PenguinClassification/PenguinClassification.html",
    "title": "Penguin Classification",
    "section": "",
    "text": "Explore\nI found helpful code and inspiration from the links below. https://seaborn.pydata.org/tutorial/introduction.html https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/classification-in-practice.html https://scikit-learn.org/stable/modules/feature_selection.html\n\nTable 1\nThe table below calculates the average flipper length and body mass for each species of penguin. From the table, we can see that Gentoo penguins appear to have the longest flippers and the greatest body mass. The Adelie and Chinstrap penguins are very close in both flipper length and body mass, with the chinstrip appeaer to be slighly larger in body mass for females where as Adelie are slightly larger in body mass for males. This indicates that aside from Gentoo penguins, it might be difficult to classify the penguins based on flipper length and body mass due to the similarity in numbers between two species of penguins.\n\ntrain.groupby(['Species', 'Sex'])[['Flipper Length (mm)', 'Body Mass (g)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n    \n    \n      \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      187.72\n      57\n      3337.28\n      57\n    \n    \n      MALE\n      192.69\n      55\n      4020.45\n      55\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      191.55\n      29\n      3514.66\n      29\n    \n    \n      MALE\n      199.67\n      27\n      3936.11\n      27\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      217.00\n      1\n      4875.00\n      1\n    \n    \n      FEMALE\n      212.93\n      42\n      4677.98\n      42\n    \n    \n      MALE\n      221.46\n      54\n      5502.31\n      54\n    \n  \n\n\n\n\n\n\nTable 2\nThe next table groups the penguins by island and counw how many of each species are at each island. We can see from the table that Adelie penguins occupy all three islands, whereas Chinstrap penguins only occupy the Dream island and Gentoo penguins only occupy the Biscoe island. The table indicates that island could be a good qualitative feature to use to classify the penguins.\n\ntrain.groupby(['Species', 'Island'])['Region'].aggregate([len]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      len\n    \n    \n      Species\n      Island\n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      Biscoe\n      35\n    \n    \n      Dream\n      41\n    \n    \n      Torgersen\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      Dream\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      Biscoe\n      101\n    \n  \n\n\n\n\n\n\nTable 3\nThe third table shows the average culmen length and culmen depth for each species of penguin broken up by gender. The table shows that Chinstrap and Gentoo penguins have similar culmen length whereas the Adelie penguins have much smaller culmen length. Culmen depth, on the other hand, is very similar between Adelie and Chinstrap penguins, and much smaller for Gentoo penguins. This indicates that together, culmen length and culmen depth can be good indicators of species as a penguin with longer culmen length and greater culmen depth would be classified as Chinstrap, a penguin with llng culmen length and small culmen depth would be classified as Gentoo, and a penguin with short culmen length and large culmen depth would be classified as Adelie.\n\ntrain.groupby(['Species', 'Sex'])[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      37.10\n      57\n      17.65\n      57\n    \n    \n      MALE\n      40.46\n      55\n      19.12\n      55\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      46.42\n      29\n      17.64\n      29\n    \n    \n      MALE\n      51.19\n      27\n      19.30\n      27\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      44.50\n      1\n      15.70\n      1\n    \n    \n      FEMALE\n      45.60\n      42\n      14.24\n      42\n    \n    \n      MALE\n      49.59\n      54\n      15.69\n      54\n    \n  \n\n\n\n\n\n\nGraph 1\nThe graph below plots the train data with culmen length on the x-axis, flipper length on the y-axis and is colored by penguin species. The graph then adds a line of best fit for each species. We can see from the graph that Gentoo penguins tend to be found in the middle to upper right quadrant of the graph, indcating greater flipper length and culmen length. Adelie penguins tend to be in the lower right quadrant of the graph indicating smaller culmen and flipper length. The chinstrap penguins tend to be in the middle of the graph showing smaller flipper length, but greater culmen length.\n\nimport seaborn as sns\n\nsns.lmplot(data=train, x=\"Culmen Length (mm)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fc88ebd5400>\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nSelecting Features\nBelow, I use a function from the scikit-learn website. The function, SelectKBest, selects the k best features and removes the rest. I started by using k = 3, as 3 is the number of features we are supposed to use to train our model, however I didn’t get any qualitative features with k = 3. Because of this, I decided to make k larger, and decided to use k = 5. From here, I printed out the 5 best features and saw that the qualitative feature, island was one of the 5 best. For that reason, I decided to use island and my qualitative feature. Next, I had to decide what two quantitative features to use. From my tables above, I decided to start with culmen length and culmen depth due to the unique way to classify the penguins when both features are combined. Additionally, culmen length and culmen depth were both in the k top features.\n\n>>> from sklearn.feature_selection import SelectKBest\n>>> from sklearn.feature_selection import f_classif\n\nmodel= SelectKBest(f_classif, k=5).fit(X_train,y_train)\n\nX_Feature_Names=X_train.columns[model.get_support()]\n\n/Users/ceceziegler/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [9] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/ceceziegler/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n\nX_Feature_Names\n\nIndex(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Island_Biscoe'],\n      dtype='object')\n\n\n\n\nFitting my model\nI attempted to use many different models from the scikit-learn page, but after fitting them and cross validating, I determined that the LogisticRegression model was the best fit. Below is my models score the first time it was fit, and below that are the scores from cross validating the model 5 times.\n\nfrom sklearn.linear_model import LogisticRegression\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(LR, X_train[cols], y_train, cv=5)\ncv_scores\n\narray([0.98076923, 1.        , 0.98039216, 1.        , 1.        ])\n\n\n\n\n\nTesting The Model!\nBelow, I test and cross validate my model on the testing data. The score is printed for both the initial fitting and the cross validating. For all but one score in the cross validating, the model returned a score of 1. This shows that our model was well fitted.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(LR, X_test[cols], y_test, cv=5)\ncv_scores\n\narray([1.        , 1.        , 1.        , 1.        , 0.92307692])\n\n\n\nPlotting Decision Regions\nhttps://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#plotting-decision-regions Below, I plot the decision regions for the testing data. The graphs are broken up by the qualitative data, which for me is islands. I used code from the link above. The colors indicate each species, and we can see from the decsion regions that our model sucessfully classified our penguins.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\nFinal Visual!\nBelow is a jointplot graph that shows both a scatter plot and bell curves of the distribution for each species regarding culmen length and culmen depth which were the two quantitative features I focused on in my model. We can see from the graphs how the penguin species are very well separated into their own groups based on culmen length and depths which indicates that these were good features to select to train our model on to properly classify penguin species.\n\nsns.jointplot(data=train, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n<seaborn.axisgrid.JointGrid at 0x7fc890f1ea30>"
  },
  {
    "objectID": "posts/LinearRegressionBlog/LinearRegBlog.html",
    "href": "posts/LinearRegressionBlog/LinearRegBlog.html",
    "title": "Linear Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/LinearRegressionBlog/LinearRegBlog.html#gradient-descent-score-over-time",
    "href": "posts/LinearRegressionBlog/LinearRegBlog.html#gradient-descent-score-over-time",
    "title": "Linear Regression",
    "section": "Gradient Descent Score over Time",
    "text": "Gradient Descent Score over Time\nBelow is a graph of how the score increases overtime with the gradient decent implementation. As we can see from the graph, the score starts off low, but increases with each iteration as the weight vector moves down the gradient. My program runs for 100 iterations, but we can see the best score is reached around 20-25 iteration mark and reamains for the rest of the iterations.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/UnsupervisedImageBlog/UnsupervisedImage.html",
    "href": "posts/UnsupervisedImageBlog/UnsupervisedImage.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here is a link to my sorce code: https://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/UnsupervisedImageBlog/SVD_Image.py"
  },
  {
    "objectID": "posts/UnsupervisedImageBlog/UnsupervisedImage.html#converting-imported-image-to-grey-scale",
    "href": "posts/UnsupervisedImageBlog/UnsupervisedImage.html#converting-imported-image-to-grey-scale",
    "title": "Unsupervised Learning",
    "section": "Converting Imported Image to Grey Scale",
    "text": "Converting Imported Image to Grey Scale\nBelow, I take my image that I imported and use the function provided in the assingment to convert my imaage to grey scale which also converts it to a matrix of values that allows us to compute singluar value decomposition. After converting the image to grey scale, I display both the original and the black and white image.\n\nfrom matplotlib import pyplot as plt\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]"
  },
  {
    "objectID": "posts/TimnitGebru/TimnitGebru.html",
    "href": "posts/TimnitGebru/TimnitGebru.html",
    "title": "Learning From Timnit Gebru",
    "section": "",
    "text": "Introduction\nTimnit Gebru grew up in Ethipoia and moved to the United States when she was 15 to flee the Eritrean-Ethiopian war. After experiencing difficulties with being allowed to live in the United States, Gebru finally settled in Sommerville Massachusetts where she attended high school. Gebru is an incrediblly influential and important figure due to her work regarding algorithmic bias in machine learning algorithms, specifically within computer vision. Through many experiences of racial bias Gebru has faced personally, she is passionate and knowelgeable on the topic. After working for world renowned tech companies including Apple and Google, Gebru has expressed the mistreatment and bias she has seen within these big tech companies. African Americans are mis-identified by computer vision technologies at a disproportionate rate. After writing a research paper for Google that detailed the potential negative effects of a new AI technology, a paper she was asked to write, Gebru left google as higher up athorities were not happy with what she wrote and didn’t want the paper published. After the poor experience with Google, Gebru doesn’t have much intrest in returning to work for a big tech company. Instead, she has started her own research intiative, Black in AI.\n\n\nDr. Gebru’s Talk\nIn the talk from Dr. Gebru, she addresses issues of fairness and ethics in technology, specifically computer vision. One of the main points Dr. Gebru brings up is how technology has done harm to black people and how black people, especially black women are under represented in the technology field. Although she mentions how technology as a whole has done harm to the black community, she speficially focusses on computer vision. Computer vision is a dangerous technology and we don’t know what it is being used for. Gebru brings up many applications that use computer vision technology in unethical way. For example, one app, Faception, profilies people based on their facial image. Other companies use facial deetction softwares during interviews in an attempt to track candidates non verbal responses and judge their performance in the interview based on what the facial dectection software finds. Not only are many facial recognition technologies unethical, but margonalized people are critisized and deprived disproportionatley. Gebru mentioned a few studies that showed that displayed this behavior. First she mentioned how police in Baltimore are unethically obtaining photographs of Black protestors and using those images to arrest them for unwarranted reasons. Next she describes how black women specifically are disproportionatley mis-identified by facial recognition technologies from major tech companies such as Google and Microsoft. Through these examples, Gebru identifies how it is not just the algorithms themselves that are bias, but there is bias in the training data. It is not an AI specific problem, but a problem of who is seen and who is heard. The bottom line is the majority of data collected is on white males. She describes how in an attempt to obtain more diverse datasets, companies unethically obtain pictures and images of people. It is difficult to obtain this data in an ethical way which is where a big issue comes into play with AI and machine learning algorthims being biased due to non-diverse datasets.\ntd;lr: Computer vision and other AI technologies have bias due to a lack of diversity in training data, and they unfairly target people who are the most margonalized and vulnerable.\n\n\nQuestion\nWhat do you think is an effective and tangible step companies can take to address issues of inequality and bias in technology and machine learning programs?"
  },
  {
    "objectID": "posts/TimnitGebru.html",
    "href": "posts/TimnitGebru.html",
    "title": "Learning From Timnit Gebru",
    "section": "",
    "text": "Introduction\nTimnit Gebru grew up in Ethipoia and moved to the United States when she was 15 to flee the Eritrean-Ethiopian war. After experiencing difficulties with being allowed to live in the United States, Gebru finally settled in Sommerville Massachusetts where she attended high school. Gebru is an incrediblly influential and important figure due to her work regarding algorithmic bias in machine learning algorithms, specifically within computer vision. Through many experiences of racial bias Gebru has faced personally, she is passionate and knowelgeable on the topic. After working for world renowned tech companies including Apple and Google, Gebru has expressed the mistreatment and bias she has seen within these big tech companies. African Americans are mis-identified by computer vision technologies at a disproportionate rate. After writing a research paper for Google that detailed the potential negative effects of a new AI technology, a paper she was asked to write, Gebru left google as higher up athorities were not happy with what she wrote and didn’t want the paper published. After the poor experience with Google, Gebru doesn’t have much intrest in returning to work for a big tech company. Instead, she has started her own research intiative, Black in AI.\n\n\nDr. Gebru’s Talk\nIn the talk from Dr. Gebru, she addresses issues of fairness and ethics in technology, specifically computer vision. One of the main points Dr. Gebru brings up is how technology has done harm to black people and how black people, especially black women are under represented in the technology field. Although she mentions how technology as a whole has done harm to the black community, she speficially focusses on computer vision. Computer vision is a dangerous technology and we don’t know what it is being used for. Gebru brings up many applications that use computer vision technology in unethical way. For example, one app, Faception, profilies people based on their facial image. Other companies use facial deetction softwares during interviews in an attempt to track candidates non verbal responses and judge their performance in the interview based on what the facial dectection software finds. Not only are many facial recognition technologies unethical, but margonalized people are critisized and deprived disproportionatley. Gebru mentioned a few studies that showed that displayed this behavior. First she mentioned how police in Baltimore are unethically obtaining photographs of Black protestors and using those images to arrest them for unwarranted reasons. Next she describes how black women specifically are disproportionatley mis-identified by facial recognition technologies from major tech companies such as Google and Microsoft. Through these examples, Gebru identifies how it is not just the algorithms themselves that are bias, but there is bias in the training data. It is not an AI specific problem, but a problem of who is seen and who is heard. The bottom line is the majority of data collected is on white males. She describes how in an attempt to obtain more diverse datasets, companies unethically obtain pictures and images of people. It is difficult to obtain this data in an ethical way which is where a big issue comes into play with AI and machine learning algorthims being biased due to non-diverse datasets.\ntd;lr: Computer vision and other AI technologies have bias due to a lack of diversity in training data, and they unfairly target people who are the most margonalized and vulnerable.\n\n\nQuestion\nWhat do you think is an effective and tangible step companies can take to address issues of inequality and bias in technology and machine learning programs?"
  },
  {
    "objectID": "posts/KernelBlog/KernelLogBlog.html",
    "href": "posts/KernelBlog/KernelLogBlog.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Introduction\nIn this blog I will implement kernel logistic regression. Kernel logistic regression uses a kernel function to calculate the weights and is often used when there are too many data points for the regular regression model to be sucessful. Throughout this blog post, I will perform many experiments that show how my kernel logistic regression model performs regularily and while changing different aspects such as the gamma and noise. One note about my code is that on each experiment I run with my model I am getting a warning error that I have been unsucessful in figuring out how to get it to go away.\n\n\nCode Explanation\nThe goal of my code is to find the values of v that minimize the empicical loss. In the fit method of my code, I first pad x and then compute a kernel matrix using the kernel function as described in the init method. The kernel matrix encodes the similarity amongst data points in the trainin set. Next, after initializing a value for v, the method uses the scipy.optimize.minimize() function to minimize the empirical risk in terms of the weight vectors, v.\nIn my model, the empirical risk is the mean of the logistic loss function which measures the difference between the models predictions and the actual labels of the training data. Empirical loss is calculated uses the logistic loss function. First, it computes the predictions using the kernel matrix and weight vector, v. Next, it plugs the predictions into the logistic loss function to compare the predicted values with the true values of y. Finally, it takes the mean of the value obtained using logistic loss.\n\n\nInitial experiments\nBelow, I run two experiements with the code provided in the blog post assignment. If my kernel logistic regression model is running properly, it should produce an accuracy score at or above 90 percent. In both experiments, I create a set of random data to run my kernel logistic regression model on. With the first set of random data, the model produces an acurracy score of 98% and with the second set of random data, the model produced an accuracy score of 97.5%. This is a strong indication that my model is performming how it is supposed to, and it is ready to move onto other experiments where aspects of the inputs are altered.\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nscore = plt.gca().set_title(f\"Accuracy = {KLR.score(X, y)}\")\nprint(KLR.score(X, y))\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n0.98\n\n\n\n\n\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nscore = plt.gca().set_title(f\"Accuracy = {KLR.score(X, y)}\")\nprint(KLR.score(X, y))\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n0.975\n\n\n\n\n\n\n\nChoosing Gamma\nNext, I will experiment with changing the values of gamma. A very large gamma value allows for a perfect training accuracy, as it overfits the training data. However, the specificity to which it creates the regions for each data point with a high gamma value results in a low testing accuracy which is ulitmately more important than achieving a perfect training score. The first example below shows a situation with an extremely high gamma value that results in a perfect training score but would result in a poor valididation score. Next, I run an experiment varying the gamma value and creating a plot that displays the contrast between the training score and the validation score as the value of gamma increases. The experiment runs across the range from 10^-2 to 10^10. The graph shows that as gamma increases until 10^1, the validation score increases. However, once we get past a gamma value of 10, the validation score rapidly decreases while the training score increases to 100%. This shows that when implementing the kernel logistic regresion model, it is better to use a smaller value of gamma even if it means a smaller training score, as it will result in a greater validation score which is what we want.\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 1000000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n1.0\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#range from 10^-2 to 10^10\ngamma_range = np.logspace(-2, 10, num=7)\n\ntrain_arr = np.zeros_like(gamma_range)\ntest_arr = np.zeros_like(gamma_range)\n\n\nfor i, gamma in enumerate(gamma_range):\n\n    model = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n\n    model.fit(X_train, y_train)\n\n    train_arr[i] = model.score(X_train, y_train)\n    \n    test_arr[i] = model.score(X_test, y_test)\n    \n    \nplt.semilogx(gamma_range, train_arr, label='Training accuracy')\nplt.semilogx(gamma_range, test_arr, label='Testing accuracy')\nplt.xlabel('Gamma')\nplt.ylabel('Accuracy')\nplt.title('Kernel Logistic Regression Performance')\nplt.legend()\nplt.show()\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\n\n\n\n\nVarying Noise\nOur next experiments will involve varying the noise when creating our data set. Varying the noise affects how spread out our moon cresents are. Below I will perform two experiments. The first is with a small noise value of 0.5. In this experiment, The validation score decreases from the beginning with a slight increase at the 10^4th gamma value before decreasing agian and leveling out at about 0.5. The training score takes an intial dip at 10^0 unlike the experiment above, before increasing and again reaching a final training score of 100%. Next, I perform an experiment with a large noise value of 100. This experiment shows different results than each of the last two graphs. In this graph, the validation score takes an intial dip to 50% at 10^0 gamma value before going back up to 60% at 10^1 gamma value and staying at 60% for the rest of the experiment. The training accuracy reached 100% at a gamma value of 10 and stayed at 100% for the remainder of the experiment. From these two experiments, we see that a smaller noise is better from smaller values of gamma, but a greater noise is better for larger gamma values.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\n\nX, y = make_moons(200, shuffle = True, noise = 0.5)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#range from 10^-2 to 10^10\ngamma_range = np.logspace(-2, 10, num=7)\n\ntrain_arr = np.zeros_like(gamma_range)\ntest_arr = np.zeros_like(gamma_range)\n\n\nfor i, gamma in enumerate(gamma_range):\n\n    model = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n\n    model.fit(X_train, y_train)\n\n    train_arr[i] = model.score(X_train, y_train)\n    \n    test_arr[i] = model.score(X_test, y_test)\n    \n    \nplt.semilogx(gamma_range, train_arr, label='Training accuracy')\nplt.semilogx(gamma_range, test_arr, label='Testing accuracy')\nplt.xlabel('Gamma')\nplt.ylabel('Accuracy')\nplt.title('Kernel Logistic Regression Performance')\nplt.legend()\nplt.show()\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\n\nX, y = make_moons(200, shuffle = True, noise = 100)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#range from 10^-2 to 10^10\ngamma_range = np.logspace(-2, 10, num=7)\n\ntrain_arr = np.zeros_like(gamma_range)\ntest_arr = np.zeros_like(gamma_range)\n\n\nfor i, gamma in enumerate(gamma_range):\n\n    model = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n\n    model.fit(X_train, y_train)\n\n    train_arr[i] = model.score(X_train, y_train)\n    \n    test_arr[i] = model.score(X_test, y_test)\n    \n    \nplt.semilogx(gamma_range, train_arr, label='Training accuracy')\nplt.semilogx(gamma_range, test_arr, label='Testing accuracy')\nplt.xlabel('Gamma')\nplt.ylabel('Accuracy')\nplt.title('Kernel Logistic Regression Performance')\nplt.legend()\nplt.show()\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\n\n\n\n\nConcentric Circles\nBelow I will implement my kernel logistic regression model with data that forms concentric circles to see how it performs. It was difficult to find values of noise and gamma that would produce a model that learned well enough to perform well on testing data. What I found is that smaller values of both gamma and noise produced better training scores without overfitting the model.\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\n\nX, y = make_circles(n_samples=100, shuffle=True, noise=0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.5)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n0.84\n\n\n\n\n\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\n\nX, y = make_circles(n_samples=100, shuffle=True, noise=0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 2)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n0.88"
  },
  {
    "objectID": "posts/ML_Final_Proj/LinearRegressionRFE.html",
    "href": "posts/ML_Final_Proj/LinearRegressionRFE.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code\nhttps://github.com/CeceZiegler1/ML_Final_Proj/blob/main/LinearRegressionAnalytic.py\nBelow, we fit our model on the x_train and and y_train datasets, and then print out the training and validation scores. This model is fitted on all 60 features in the dataset. We can see from the scores, that it is not performing great, as a validation score below 50% indicates we could do better by just randomly selecting. We are going to perform a recursive feature elimination that we also implemented in our source code to see if we can find the optimal number of features to use to obtain the best score.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\n\n#Seeing how the model performs without RFE\n\nLR = LinearRegressionAnalytic()\nLR.fit(x_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(x_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(x_test, y_test).round(4)}\")\n\nTraining score = 0.5258\nValidation score = 0.4605\n\n\nBelow, we create an array to store the score that is produced with each different number of features used in the model as selected by our RFE. We use a for loop to loop through each value from 1-60 and display the score at each iteration in a graph. What we see from the graph, is even with using fewer features, our score never gets above around 45%. The best scores come around 12-15 features and 55-60 feautres. Even still, the scores at these points aren’t very good. Although we were hoping linear regression would perform well on our dataset, it doesn’t appear to be the case. Because of this, we are going to implement a random forest tree to see if we can obtain a better validation score on our dataset.\n\n\n# compute the score for each value of k\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\n\n[33, 34, 3, 35, 37, 6, 39, 2, 43, 45, 14, 19, 23, 28]\n\n\nBelow, we will show the 13 most important features as obtained through our rfe.\n\ndata.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\n\nx_train = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train, y_train)\nlr.score(x_test, y_test)\n\n0.386607442390802"
  },
  {
    "objectID": "posts/ML_Final_Proj/RFE.html",
    "href": "posts/ML_Final_Proj/RFE.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nmodel = LinearRegression()\nfeat_select = RFE(model, n_features_to_select = 10, step = 1)\n\nWhen using Recursive Feature Elimination, we need to select a few variables. First, we need to select what model we want to use. Since we are using multiple linear regression, we select linear regression. We also need to select how many features we want to use, and how many we want to remove during each recursion. These we chose arbitrarily for this example to be 10 and 1 respectively.\nIf we then run our RFE, we find that it classified our features as True or False, where True refers to features it selected and False ones it did not.\n\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True, False, False, False, False,  True,  True,  True,\n       False,  True,  True, False,  True])\n\n\nFrom this we can see that it selected features :42, 43, 46, 47, 52, 53, 54, 56, 57 and 59. This means that the data set we would train our model on is the following\n\ndata.iloc[:,[42,43,46,47,52,53,54,56,57,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      1850.5878\n    \n  \n\n637 rows × 10 columns\n\n\n\nHowever, 10 was arbitrarily chosen, and may not be the best choice. If we run a series of RFE’s which select for 1 more feature than the last we can then plot these values to find our optimal choice for the number of features.\n\n  \nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\n# define data values\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Linear Regression\")\nplt.show() \n\n\n\n\nFrom this chart we can see that we will be able to use a model with 15 features without losing too much accuracy, so we will next use RFE to find out the variables we want to use.\n\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True])\n\n\nThese refer to the following data set:\n\ndata.iloc[:,[42,43,46,47,48,49,50,51,52,53,54,56,57,58,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfx\n      min_lfx\n      max_lfy\n      min_lfy\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfy\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      121.2052\n      -885.2729\n      130.9304\n      -414.4391\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      545.3695\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      111.2187\n      -767.0614\n      128.0167\n      -475.8343\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      603.8510\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      178.4852\n      -827.7215\n      161.8112\n      -437.5895\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      599.4007\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      170.5486\n      -903.5394\n      187.1682\n      -430.2591\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      617.4273\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      176.3782\n      -939.6424\n      177.3536\n      -420.4205\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      597.7741\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      69.2794\n      -693.0041\n      121.9773\n      -342.0296\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      464.0069\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      60.6210\n      -669.8240\n      129.1773\n      -342.2472\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      471.4245\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      56.2369\n      -670.0392\n      111.4454\n      -329.7390\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      441.1844\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      67.6610\n      -734.0559\n      149.3230\n      -383.7818\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      533.1048\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      91.0192\n      -695.4843\n      150.5726\n      -252.4603\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      403.0329\n      1850.5878\n    \n  \n\n637 rows × 15 columns\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\n\nHowever, linear regression is not the only model that can deal with continous data. We can also use random forest regression. If we use random forest regression, and select 15 features again, we get a higher score than we did with linear regression. We also get slightly different features selected.\n\nfeat_select.fit(x_train, y_train)\nprint(feat_select.score(x_test, y_test))\nfeat_select.support_\n\nHowever, we don’t know if 15 is the optimal amount of features. As such, we can follow the same route we did with linear regression and graph our scores.\n\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\nThis shows us that any number of features over 10 will give us a model as good as using more. Therefore, to keep things similar between our two models we can use 15 features."
  },
  {
    "objectID": "posts/ML_Final_Proj/RandomForestRegressorAnalysis.html",
    "href": "posts/ML_Final_Proj/RandomForestRegressorAnalysis.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nfrom RandomForestRegressor import RandomForest\n\n\nrf = RandomForest()\n\n\nrf.fit(x_train, y_train, 1000, 500)\n\nWe also chose to implement and train a second model on our data. We implemented Random Forest Regression, since it is a very powerful model for making predictions with continous data. When we train this model on our whole data set, we get a much better validation score than with linear regression. However, just like with linear regression we can use feature reduction to reduce overfitting.\n\nrf.score(x_test, y_test)\n\n0.5699951488283441\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\n\n\n\nWe can see that this machine doesn’t suffer as much from overfitting, and has a higher validation score than Linear Regression.\n\nx_train = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train, y_train, 1000,500)\nrf.score(x_test, y_test)\n\nUsing the features we found from our RFE we get this score."
  },
  {
    "objectID": "posts/ML_Final_Proj/index.html",
    "href": "posts/ML_Final_Proj/index.html",
    "title": "Predicting Bat Speed",
    "section": "",
    "text": "Formal Introduction to Topic\nFor our final project, we are using data from the Open Biomechanics Project (OBP) driveline baseball research. Our data captures the biomechanical breakdown of a baseball player’s swing by measuring forces produced by different body parts in three dimensions over exact instances in time, for example, at instance X, player 103’s 2nd swing has a max lead hip force in the y direction of 111.73. The data was captured using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. The available data is rather robust, accounting for every piece of information that could be responsible for a baseball swing.\nFor our project, our goal is to create a machine learning model that uses this OBP data to identify the most important features of a player’s swing when generating bat speed, and then use those features to accurately predict a player’s bat speed. By comparing an athlete’s true bat speed to their predicted bat speed based on our model, the player could identify how efficiently they are swinging. We hope that this model could be used by baseball players and coaches to address the unique aspects of a player’s swing that could contribute to a higher bat speed, which in turn, would the players reach their fullest potential based on where their inefficiencies lie. Our project can be broken down into two main processes: Identifying the key features that contribute to bat speed. Creating a model that uses the key features to predict bat speed. For the first step, we have decided to run a Recursive Feature Elimination(RFE) on the 60 potential features from our dataset to pull out a smaller number of strong predictive features to use in our model. Next, using those select features, we will run a regression analysis to create a model that can be used to predict a player’s bat speed. Let’s take a closer look at these analyses.\n\n\nValues Statement\n\nsee assignment post for what to include here\n\n\n\nMaterials and Methods:\n\n\nData\nFrom the OBP dataset, we will be focusing on baseball-hitting data, specifically force plate and joint velocity to predict bat speed. The original datasets can be found here. Driveline baseball research collected this data using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. Originally, both the force plate and joint velocity datasets had over 1,000,000 observations, with each individual swing including thousands of observations because the swing was broken down by milliseconds. We felt it was unnecessary to keep the time aspect of the dataset, as the velocities produced for each feature variable were very similar from millisecond to millisecond, and the large datasets were difficult to work with. To get rid of the time component and obtain a more reasonably sized data set, we found the minimum, maximum and range of each feature variable in the dataset for every swing. Each swing is labeled by session_swing in our dataset, and each row is a different swing. The session swing is labeled by player ID and swing number, for example, session_swing 111_3 is player 111’s third swing. Not all players have the same number of swings in the dataset, but we don’t think this should have any impact on our results. After eliminating the time aspect, each swing has 60 potential feature variables. The 60 feature variables include the min, max and range of the forces produced by many different body parts in the x, y and z directions during a player’s swing. Some examples include lead shoulder which is the player’s shoulder closest to the pitcher, and rear hip which is the player’s hip furthest from the pitcher.\nOur data possesses some limitations as it exclusively represents male baseball players and doesn’t include any data from female softball players. We think it would be interesting for Driveline baseball research to expand to softball to eliminate some of the gender bias they have inadvertently caused.\n\n\nRecursive Feature Elimination\nRecursive Feature Elimination, or RFE is a recursive algorithm that is used to find the most impactful features out of a large feature set. This is accomplished by training machine learning on all the features, and then removing the least impactful features. This process is repeated with the smaller feature set, until the feature set is of the desired size. This can help prevent overfitting and allow for easier use and training. This does require that the model it is being used to select features has a way to calculate the effect of features, which means that it won’t work for every model, or some determinator has to be created for it to be used. Another drawback is that unless proper testing is done to find out the amount of impactful features, the accuracy can be diminished beyond the benefits of avoiding overfitting.\n\n\nMultiple Linear Regression\nBecause our project is concerned with predicting bat speed, we require a numeric prediction model, rather than a classification prediction model. We decided to use Multiple Linear Regression, which allows us to take two or more features in a dataset to predict a single numeric dependent variable, bat speed. Multiple Linear Regression differs from regular Linear Regression in that you can use more than one feature to predict the target variable. Once built, we can isolate each individual feature and evaluate its impact on the target variable.\nWith our linear regression model, we will be using the Mean Squared Error (MSE) loss function to determine the accuracy and performance of our model.\n\n\nRandom Forest Regression\nRandom Forest Regression is a technique that creates multiple decision trees and averages their outputs to give a final result that often has a high prediction/classification rate. The process involves the user selecting the amount, n, of decision trees to be created, then using the bootstrapping method to randomly select k data samples from the training set. (Bootstrapping is simply the process of randomly selecting subsets from a dataset over a certain number of iterations and a certain number of variables, then the results from each subset are averaged together which returns a more powerful result.) Then, create n decision trees using different random k values. Using the decision trees, predict regression results that can be used on unseen data. Finally, the regression results are all averaged together, returning a final regression output. Random Forests generally provide high accuracy for feature selection as it generates uncorrelated decision trees built by choosing a random set of features for each tree.\n\n\nVariable Overview\nThe features which we have created our data set with fall into two main categories: biomechanics data and force plate data. Beginning with the biomechanics data, we have a set of joints and their associated angular velocities in three planes of motion. We have information on lead hip, lead shoulder, rear hip, rear shoulder, pelvis, torso, and the torso-pelvis complex. For each of these joints, we calculated the range of velocities and maximum velocities for each swing captured. With the force plate data, the lead foot and rear foot are independently observed, and the data is split among the three planes of motion along the x, y, and z axes. For each pairing of foot and plane of motion, we calculated the minimum, maximum, and range of the force produced.\n\n\nThe Process\n\nStep 1: Cleaning the Data\nOur original dataset contained over 1,000,000 observations that were grouped by session_swing. Each swing contained hundreds of observations that analyzed a variety of features over time (~0.0028 seconds between captures). For our project, we wanted to remove this time aspect and instead create a simplified dataset that contained the minimum, maximum, and range values of the features of interest for each swing.\nTo do so, we imported our dataset in R and grouped it by the session_swing variable, and, by using base R functions, calculated the minimum, maximum, and range of each feature variable of interest. We repeated this for the force plate dataset and joint velocity dataset, then used left-join to combine the two datasets to create a conglomerate dataset with all potential predictive features for each session_swing variable.\nWe then added our target vector, max bat speed, from POI_metrics.csv to create our fill dataset that includes our target vector.\nThis process allowed us to get reduce the size of our dataset from over 1,000,000 observations to 665 session_swings.\n\n\nStep 2: RFE Feature Selection\nWe used SKLearn’s RFE feature collection class, which can be found here.\nThe RFE model from the SKLearn class allowed us to gain the base understanding we needed to implement our own version of RFE. After reading through the API and playing around with the RFE feature from SKLearn, we decided to implement our own version of RFE to use with the linear regression model we also implemented. Our RFE function tkaes in three parameters: a feature matrix, X, a target vetor, y, and the number of features we want to be selected, k. The function uses a nested for loop to run through all values i in 1:k and at each iteration, j, checks the weight of all the remaining features that were fit on the linear regression model. From here, the best features are selected as the features with the minimum absolutle value of weight. We use this function in conjunction with our linear regression model to find the number of feautres within our dataset that most accuratley predict the batspeed of a player.\n\n\nStep 3: Building the Models — Multiple Linear Regression and Random Forest\nWe decided to implement our own linear regression model similar to the work we did in class, selecting just the analytic version. We were able to pull out score and predict functions from our previous blog post implementation. We had to modify our fit function by adding a regualarization term to the weight vector to help avoid over/under-fitting the model.\n\n\nStep 4: Testing the Models: Linear Regression and Random Forest\nTo test and train the models, we used an 80/20 train/test split. For both models, we ran a loop to show our the training and testing scores while increasing the number of selected features for the recursive feature elimination model. Once we identified the optimal range of features to use on the Multiple Linear Regression and Random Forest Regression models, we created subsets of our training and testing data to contain the selected features. Finally, we trained and tested our models on the data subsets (80 train/20 split).\n\n\n\nResults and Conclusion\n\nLinear Regression\nBelow shows the effect of increasing the number of features on the accuracy scores produced by the Linear Regression model during RFE. As we can see, our training and testing accuracy scores tend to increase as the number of features increase.\nWe noticed that our model tends to have a higher testing accuracy when the model is ran on fewer features, and has the best training score when the model uses all 60 features. However, because we wanted to identify 10-20 key features, we decided to train and test our model on the 13 best features, which we selected as value that yields the second best testing score.\nThe table shows the 13 most important features as selected by our RFE function. These results were quite unexpected, including values like min_rfx and min_rfy as selected variables of importance. min_rfx and min_rfy represent the minimum rear force produced in the x and y directions, which essentially represent the load, or backwards movement prior to the actual swing itself. Other variables make sense as being some of the most important features, such as range_lead_hip_z, and max_torso_pelvis_x as these are body parts that are essitial in creating the rotational force of a swing to help produce a better bat speed.\nUsing a subset of these 13 features, we trained and tested our model, which produced a testing accuracy score of 38.7%. Unfortunately, this low accuracy indicates that our model isn’t performing as we had hoped. It could mean that our data doesn’t have a strong linear seperability which indicates there is nothing wrong with our model, but rather it isn’t the best model option for our data. Becasue of this, we decided to see if we could produce stronger results by using the Random Forest Regression model.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"biomechanics_dataset_v1.csv\") \n\nnp.random.seed(1)\nX = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n\n\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Scores Produced by Linear Regression Model During RFE\")\nplt.show()\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]])\n\n\nx_train_lin = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test_lin = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train_lin, y_train)\nprint(\"Testing Accuracy of Subset:\", lr.score(x_test_lin, y_test))\n\n\n\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\nTesting Accuracy of Subset: 0.386607442390802\n\n\n\n\nRandom Forest Regression\nBelow is a graph that displays the training and testing scores produced by the Random Rorest Regression model when ran with RFE over all of the features. As we can see from the graph, this model performed much better than the Linear Regression model. Unlike the Linear Regression model, the Random Rorest model reached a peak around 10 features and maintained consistency at that score, whereas the Linear Regression model had increased variance between scores across all of the features.\nBecause the Random Forest Regression model creates a series of trees using the bootstrapping method, we expected the model to have a better training accuracy and weren’t as concerned with overfitting. We also expected it to have a better accuracy score than the Linear Regression model because it can capture non-linear relationships, which we expect our data to have due to the poor performance of the Multiple Linear Regression model. Aditionally, because of the way the trees are built, the random forest model is less likely to be heavily affected by outliers in the data which will allow it to have a better testing accuracy score.\nWe found that the Random Rorest Regression model had the higher training and validation scores than the Linear Regresion model. The training score reached nearly 100% when ran with more than 10 features. The testing accuracy reached around 65% at 10 features and had a slight increase as the number of features increased. To compare this model with the Linear Regression model, we chose the 15 most important features to test and train the Random Forest model. After training and testing our model on the subset of 15 features, we got a testing accuracy score of 55.4%, which is significantly better than our multiple linear regression model. Considering there is no option for our model to randomly guess since we are predicting a continuous numeric value, we are satisfied with the amount our random forest model learned.\nInterestingly, there is only one feature that the Linear Regression model and the Random Forest Regression model both selected: max_rear_shoulder_y. This feature captures the top hand on the bat, so if the shoulder isn’t moving in the swing, it will hinder the ability to produce enough rotational force from the torso.\nWe were suprised to find that, of the 13 features selected by the Linear Regression model and of the 15 features selected by the Random Forest Regression model, there weren’t more similarities amongst the selected features between the two models. We were hoping to discover a few select features that both models identified as important features, and were interested to find that the majority of the selected features were different.\nWe hypothesize that the dimentionality of the forces produced by each body part may be a factor that contributes toward the challenge of distinguishing significant features. For example, in the Linear Regression model, max_toros_y was selected as an important feautre, and in the Random Forest Regression model, max_torso_z was selected as an important feature. This indicates that the max force produced by the torso is important when creating and predicting bat speed. If we were to run this experiment again, we may try getting rid of the x, y and z components and just use the average of the forces produced by that body part.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores_test = []\nscores_train = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores_test.append(estm.score(x_test,y_test))\n    scores_train.append(estm.score(x_train,y_train))\n    \nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores_test, label='Testing accuracy')  # Plot the chart\nplt.plot(x_val, scores_train, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy Score\")\nplt.title(\"Random Forest Regression\")\nplt.legend()\nplt.show() \n\n\nfrom RandomForestRegressor import RandomForest\nrf = RandomForest()\n\nx_train_rf = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test_rf = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train_rf, y_train, 1000,500)\nprint(rf.score(x_test_rf, y_test))\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]])\n\n0.5541368058167476\n\n\n\n\n\n\n  \n    \n      \n      range_lead_hip_x\n      range_pelvis_y\n      range_rear_shoulder_x\n      range_torso_z\n      range_torso_pelvis_z\n      max_lead_shoulder_z\n      max_pelvis_z\n      max_rear_shoulder_y\n      max_torso_z\n      max_rfz\n      max_lfx\n      range_rfy\n      range_rfz\n      range_lfy\n    \n  \n  \n    \n      0\n      590.6812\n      371.0611\n      838.0101\n      848.3957\n      743.5585\n      617.1386\n      733.6451\n      514.1198\n      775.7749\n      1101.3711\n      121.2052\n      240.6389\n      1052.5648\n      545.3695\n    \n    \n      1\n      536.1970\n      393.4254\n      947.9660\n      814.2556\n      642.8480\n      751.1699\n      799.8748\n      535.2822\n      775.3766\n      1092.3006\n      111.2187\n      297.5680\n      1040.9895\n      603.8510\n    \n    \n      2\n      586.8320\n      396.8130\n      801.1592\n      823.2495\n      853.6754\n      723.6880\n      740.7065\n      472.9189\n      793.0441\n      1117.9434\n      178.4852\n      351.2961\n      1002.5322\n      599.4007\n    \n    \n      3\n      628.4384\n      402.3244\n      958.8471\n      870.6640\n      541.5395\n      810.9479\n      741.3719\n      484.2663\n      819.9890\n      1102.4140\n      170.5486\n      344.0314\n      1094.4491\n      617.4273\n    \n    \n      4\n      595.3172\n      348.1626\n      840.4242\n      809.9368\n      756.6446\n      862.6313\n      770.4950\n      502.2202\n      774.5865\n      1119.0327\n      176.3782\n      262.0008\n      1100.7345\n      597.7741\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      825.4631\n      350.8260\n      1015.7101\n      919.6701\n      587.4149\n      838.6431\n      679.5463\n      488.3580\n      859.0192\n      947.5325\n      69.2794\n      380.5133\n      916.4106\n      464.0069\n    \n    \n      633\n      768.7166\n      377.7951\n      1020.7830\n      910.0289\n      716.7429\n      799.7780\n      715.3288\n      477.7701\n      846.6447\n      958.0700\n      60.6210\n      365.7219\n      931.9138\n      471.4245\n    \n    \n      634\n      667.8735\n      366.3885\n      978.3792\n      880.3159\n      516.4786\n      815.2906\n      701.0455\n      407.3251\n      820.1794\n      998.6667\n      56.2369\n      331.5945\n      954.3035\n      441.1844\n    \n    \n      635\n      698.0434\n      390.7154\n      1029.8385\n      896.8795\n      616.6955\n      604.2217\n      681.9455\n      463.8467\n      838.6638\n      939.1254\n      67.6610\n      383.2294\n      909.9346\n      533.1048\n    \n    \n      636\n      764.1730\n      348.0633\n      1024.9879\n      907.0956\n      606.3088\n      830.5895\n      677.4534\n      505.9937\n      846.7994\n      935.7064\n      91.0192\n      346.1590\n      916.5282\n      403.0329\n    \n  \n\n637 rows × 14 columns\n\n\n\n\n\nConcluding Discussion\nOverall, our project was sucessful, as we built two models and an RFE function to help us determine the most important features of a swing while predicting bat speed. When we formulated the idea for our project idea, our goal was to deliver python source code that we constructed, along with two jupyter notebooks. One that contained our intial exploration, and one that held our final write up and experiments. We were sucessful in meeting this goal, as we finished with more than two jupyter notebooks, and two python source code files that contained our linear regression with RFE and our random forest model.\nIf we had more time, we would work on finding ways to improve the accuracy of our models. One idea we have to improve accuracy score is to get rid of the dimension factor, as we mentioned above, so each biomechanic force only has one representation instead of three. We hope this would help our models narrow down the important features and produce a better accuracy score. Additionally, we would like to bring in more data to train and test our model on. DriveLine baseball technology is relatively new, so the data is sparse. If the technology was more accesible, we could have more data which would allow our model to improve its success.\n\n\n\nBias\nWe created a model that predicts a player’s bat speed with approximately 65% accuracy, using around 10 features. As with any algorithm, we must consider any assumptions or biased data that could have resulted in systematic error throughout our process. Because our training data came from the Driveline Research and Development, our model was only trained on the players with close proximity or eligibility to their biomechanics facility. According to the U.S. Census, 43.3% of the population of Kent, Washington (home of Driveline R&D) are white individuals, contrasted with 12.2% of the population being black individuals. While the data’s goal is to highlight specific bodily forces and movements that contribute to predicting bat speed rather than demographics like race, age, height, or weight, we must acknowledge that this data is most likely skewed toward white individuals and could be less accurate in predicting the bat speed of players of different races.\nAdditionally, we must highlight that baseball is a male-dominated sport, with the rare exception of a few women playing the sport — see article on Alexis Hopkins. While sports are typically gender segregated for the sake of “fairness” and an acknowledgment that male and female bodies are inherently different and will perform as such, factors like absolute strength and size are not as important in the sport of baseball, as they might be in gender-segregated sports like football and soccer. Rather, the Women’s Sports Foundation explains that baseball involves skills that are combinations of timing, coordination, strength, knowledge of the game, strategies, and control, and argues that bat speed and bat control are more important than absolute strength.\nYet, despite all of this, the Driveline R&D data only contains the biomechanics of male batters. Therefore, if our model were to be improved and implemented, it would only perpetuate the segregation of men and women in this sport. If the data to improve a player’s bat speed can only improve male players, women will continue to be left in the dust.\nDriveline Baseball describes its mission as developing the future of baseball training by finding industry-leading insights; all while publishing those insights back to the baseball community where they belong. However, because baseball is a historically white and male-dominated sport, the “insights” that will be found will only contribute to solidifying that the “baseball community” remains dominated by players that fit those demographics.\nIt is our duty to expand this research and development into more marginalized player communities, such as female athletes and athletes of other races. Then, we can use these insights to create unique training programs that empower and embrace their unique features and help them become the best athletes they can be.\n\n\nApplication\n\nHow could this be used by other teams/sports/groups\n\n\n\nGroup Contribution Statement\nCece Ziegler: Helped with data cleaning. Built RFE function and Linear Regression model. Performed RFE and model experiments. Led writing “Results and Conclusion” sections.\nDavid Byrne: Introduced topic. Managed data cleaning. Led writing of “Abstract and Overview of Significance of Topic”, “Data”, “Variable Overview”, “Values Statement”, and “Application” sections.\nJulia Fairbank: Led writing of “Formal Introduction to Topic”, “Recursive Feature Elimination”, “Multiple Linear Regression”, “Random Forest Regression”, “The Process”, and “Bias” sections.\nSam Ehrsam: Conducted initial RFE experiments with SKLearn library. Built Random Forest Regression model. Performed RFE experiments."
  }
]