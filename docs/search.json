[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This blog post illustrates my implementation of penguin classification.\n\n\n\n\n\n\nMar 7, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the logistic regression, using gradient decent.\n\n\n\n\n\n\nMar 2, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the perceptron algorithm, along with test cases to show its performance.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html",
    "href": "posts/perceptron_notebook/PerceptronBlog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Here is a link to my source code.\nhttps://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/perceptron_notebook/perceptron.py"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#graph-of-change-in-accuracy-with-each-iteration",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#graph-of-change-in-accuracy-with-each-iteration",
    "title": "Perceptron Blog",
    "section": "Graph of Change in Accuracy with each Iteration",
    "text": "Graph of Change in Accuracy with each Iteration\nThis graph shows the change in accuracy of the algorithm with each iteration. We can see here that it didn’t take the algorithm very long to converge to zero and reach an accuracy of 100%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "title": "Perceptron Blog",
    "section": "The graph below shows the accuracy over time of non linearly seperable data",
    "text": "The graph below shows the accuracy over time of non linearly seperable data\nWe can see from this graph that the accuracy fluctuates a lot with each iteration as the algorithm attempts to accuratley sort out the non-linearly separable data. It is unable to reach an accuracy of 100% with the best accuracy it reached coming at around 60%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "title": "Perceptron Blog",
    "section": "Below is a graph of the change in accuracy over time for data with more than 5 features",
    "text": "Below is a graph of the change in accuracy over time for data with more than 5 features\nWe can see from this graph, similar to our 2D linearly separable data, the algorithm did not take a long time to converge to zero and reach an accuracy of 100%\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\n#np.random.seed(12345)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n\ny = 2*y -1\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n0.0"
  },
  {
    "objectID": "CeceZieglerRepo/posts/example-blog-post/index.html",
    "href": "CeceZieglerRepo/posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "CeceZieglerRepo/posts/example-blog-post/index.html#math",
    "href": "CeceZieglerRepo/posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "CeceZieglerRepo/index.html",
    "href": "CeceZieglerRepo/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CeceZieglerRepo/about.html",
    "href": "CeceZieglerRepo/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "CeceZiegler1.github.io/posts/example-blog-post/index.html",
    "href": "CeceZiegler1.github.io/posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "CeceZiegler1.github.io/posts/example-blog-post/index.html#math",
    "href": "CeceZiegler1.github.io/posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "CeceZiegler1.github.io/index.html",
    "href": "CeceZiegler1.github.io/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CeceZiegler1.github.io/about.html",
    "href": "CeceZiegler1.github.io/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html",
    "href": "posts/perceptron_notebook/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Here is a link to my source code.\nhttps://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/perceptron_notebook/perceptron.py"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#graph-of-change-in-accuracy-with-each-iteration",
    "href": "posts/perceptron_notebook/index.html#graph-of-change-in-accuracy-with-each-iteration",
    "title": "Perceptron Blog",
    "section": "Graph of Change in Accuracy with each Iteration",
    "text": "Graph of Change in Accuracy with each Iteration\nThis graph shows the change in accuracy of the algorithm with each iteration. We can see here that it didn’t take the algorithm very long to converge to zero and reach an accuracy of 100%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "href": "posts/perceptron_notebook/index.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "title": "Perceptron Blog",
    "section": "The graph below shows the accuracy over time of non linearly seperable data",
    "text": "The graph below shows the accuracy over time of non linearly seperable data\nWe can see from this graph that the accuracy fluctuates a lot with each iteration as the algorithm attempts to accuratley sort out the non-linearly separable data. It is unable to reach an accuracy of 100% with the best accuracy it reached coming at around 60%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "href": "posts/perceptron_notebook/index.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "title": "Perceptron Blog",
    "section": "Below is a graph of the change in accuracy over time for data with more than 5 features",
    "text": "Below is a graph of the change in accuracy over time for data with more than 5 features\nWe can see from this graph, similar to our 2D linearly separable data, the algorithm did not take a long time to converge to zero and reach an accuracy of 100%\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\n#np.random.seed(12345)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n\ny = 2*y -1\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n0.0"
  },
  {
    "objectID": "posts/gradient_blog/LogRegBlog.html",
    "href": "posts/gradient_blog/LogRegBlog.html",
    "title": "Gradient Decent",
    "section": "",
    "text": "Gradient Decent Implementation\nBelow, I show an example of my implementation of logistic regression using gradient decent. I make data and create a plot to show where my line separated the data. I then print out the loss and the accuracy to show how well the model performed. On average, my model averaged a loss between 0.15-0.19 which makes sense as the data is not linearly seperable, which makes this a reasonable loss.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nPrinting out Gradient Decent Loss\n\n#Loss\nprint(LR.loss)\n\n0.15683836141209298\n\n\n\n\n\nStochastic Gradient Implementation\nBelow is an example of a test of my stochastic gradient implementation. The stochastic gradient is similar to regular gradient decent, however it uses batch sizes to partiton the data and computes the gradient of the batch instead of the whole gradient at once. As we can see from the example below, depending on factors such as the size of the batch, stochastic gradient can help us minimize the loss. After running the regular gradient decent and the stochastic gradient decent many times each, the stochastic gradient decent averages a loss of about .13-.16 which is slightly lower than the loss of regular graident decent. Along with having on average a slightly better loss, the stochastic gradient method converges to a minimum faster than regular gradient decent\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, momentum = False, batch_size = 10)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LRS.loss)\n\n0.1364397783931314\n\n\n\n\nMomentum with Stochastic Gradient\nBelow is an example of a test using my stochastic gradient implementation including momentum. The model is the exact same as the stochastic gradient model, however one parameter is changed. When calling the fit method for regular stochastic gradient, I set the momentum parameter = to false. This indicated to set the coeficent beta to 0, meaning momentum won’t be added in when the weight vector is updated. However, when the momentum parameter is true, this indicates to include momentum, thus setting the beta coefficient to 0.8. Adding in momentum accelerates the process in which we are making our way down the gradient toward the minimum, thus helping to converge to the minimum faster. In this example, we also print out the loss which in this specific example was a solid loss at only 0.13, but the loss tends to remain in the same range as regular stochastic gradient\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, momentum = True, batch_size = 10)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LRS.loss)\n\n0.1364397783931314\n\n\n\n\nLoss History Over Time\nBelow is a graph of each of the three models described above showing their loss history over each epoch/iteration. As we can see from the graph, regular gradient decent takes much longer to converge than stochastic gradient and stochastic gradient with momentum. Both the stochastic with momentum took 100 iteration, and stochastic without momentum took 200 iterations to converge whereas the regular gradient decent took 2100 iterations. This is a very significant difference in time and displays how stochastic gradient implementations can converge faster. We are also able to see from this how momentum helps speed up the time of convergence, as the model that included momentum took 100 less iteration to converge.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n100\n200\n2100\n\n\n\n\n\n\n\nAlpha too big to Converge\nBelow is an example in which the size of the alpha is too large which causes the model to be unable to converge to the minimum. In this test example, I set the alpha to 35 and the max_epochs to 100. Although the model still completed running, it’s loss was 0.26 which is much higher than we want and higher than any of the above model examples. This shows that the alpha size was too large, and the model was unable to converge to the proper minimum.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 30, max_epochs = 100)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR.loss\n\n0.2608236468870702\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 2, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient small batch\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 20, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient large batch\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n#This shows that small batch size takes longer to converge, 100 iterations for large batch compared to 600 for small batch\n\n100\n300\n\n\n\n\n\n\n\nExample showing batch size influencing convergence speed.\nIn the graph above, we can see how batch size affects the speed of convergence. Here, we show the convergence of two regular stochastic gradients, one with a batch size of 2, and the other with a batch size of 20. The function with a batch size of 20 took 100 iterations to converge, whereas the function with a batch size of 2 took 300 iterations to converge. This is three times the amount of iterations which shows that having a small batch size takes a longer time to converge.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 500, \n                  momentum = True, \n                  batch_size = 250, \n                  alpha = 0.1) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 500, \n                  momentum = False, \n                  batch_size = 250, \n                  alpha = 0.1)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n#Momentum converges faster when the batch size is large\n\n500\n1000\n\n\n\n\n\n\n\nExample: Momentum Converging Faster than Regular Stochastic:\nThe graph above shows an example of when momentum significantly speeds up convergence. In this example, we have a large batch size. With the large batch size, the stochastic momentum gradient only takes 500 iterations to converge, whereas the regular stochastic gradient takes 1000 iterations to converge. This is double the amount of iterations, showing that when the batch size is large, momentum helps the stochastic gradeient converge faster."
  },
  {
    "objectID": "posts/PenguinClassification/PenguinClassification.html",
    "href": "posts/PenguinClassification/PenguinClassification.html",
    "title": "Penguin Classification",
    "section": "",
    "text": "Explore\nI found helpful code and inspiration from the links below. https://seaborn.pydata.org/tutorial/introduction.html https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/classification-in-practice.html https://scikit-learn.org/stable/modules/feature_selection.html\n\nTable 1\nThe table below calculates the average flipper length and body mass for each species of penguin. From the table, we can see that Gentoo penguins appear to have the longest flippers and the greatest body mass. The Adelie and Chinstrap penguins are very close in both flipper length and body mass, with the chinstrip appeaer to be slighly larger in body mass for females where as Adelie are slightly larger in body mass for males. This indicates that aside from Gentoo penguins, it might be difficult to classify the penguins based on flipper length and body mass due to the similarity in numbers between two species of penguins.\n\ntrain.groupby(['Species', 'Sex'])[['Flipper Length (mm)', 'Body Mass (g)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n    \n    \n      \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      187.72\n      57\n      3337.28\n      57\n    \n    \n      MALE\n      192.69\n      55\n      4020.45\n      55\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      191.55\n      29\n      3514.66\n      29\n    \n    \n      MALE\n      199.67\n      27\n      3936.11\n      27\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      217.00\n      1\n      4875.00\n      1\n    \n    \n      FEMALE\n      212.93\n      42\n      4677.98\n      42\n    \n    \n      MALE\n      221.46\n      54\n      5502.31\n      54\n    \n  \n\n\n\n\n\n\nTable 2\nThe next table groups the penguins by island and counw how many of each species are at each island. We can see from the table that Adelie penguins occupy all three islands, whereas Chinstrap penguins only occupy the Dream island and Gentoo penguins only occupy the Biscoe island. The table indicates that island could be a good qualitative feature to use to classify the penguins.\n\ntrain.groupby(['Species', 'Island'])['Region'].aggregate([len]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      len\n    \n    \n      Species\n      Island\n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      Biscoe\n      35\n    \n    \n      Dream\n      41\n    \n    \n      Torgersen\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      Dream\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      Biscoe\n      101\n    \n  \n\n\n\n\n\n\nTable 3\nThe third table shows the average culmen length and culmen depth for each species of penguin broken up by gender. The table shows that Chinstrap and Gentoo penguins have similar culmen length whereas the Adelie penguins have much smaller culmen length. Culmen depth, on the other hand, is very similar between Adelie and Chinstrap penguins, and much smaller for Gentoo penguins. This indicates that together, culmen length and culmen depth can be good indicators of species as a penguin with longer culmen length and greater culmen depth would be classified as Chinstrap, a penguin with llng culmen length and small culmen depth would be classified as Gentoo, and a penguin with short culmen length and large culmen depth would be classified as Adelie.\n\ntrain.groupby(['Species', 'Sex'])[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      37.10\n      57\n      17.65\n      57\n    \n    \n      MALE\n      40.46\n      55\n      19.12\n      55\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      46.42\n      29\n      17.64\n      29\n    \n    \n      MALE\n      51.19\n      27\n      19.30\n      27\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      44.50\n      1\n      15.70\n      1\n    \n    \n      FEMALE\n      45.60\n      42\n      14.24\n      42\n    \n    \n      MALE\n      49.59\n      54\n      15.69\n      54\n    \n  \n\n\n\n\n\n\nGraph 1\nThe graph below plots the train data with culmen length on the x-axis, flipper length on the y-axis and is colored by penguin species. The graph then adds a line of best fit for each species. We can see from the graph that Gentoo penguins tend to be found in the middle to upper right quadrant of the graph, indcating greater flipper length and culmen length. Adelie penguins tend to be in the lower right quadrant of the graph indicating smaller culmen and flipper length. The chinstrap penguins tend to be in the middle of the graph showing smaller flipper length, but greater culmen length.\n\nimport seaborn as sns\n\nsns.lmplot(data=train, x=\"Culmen Length (mm)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fc88ebd5400>\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nSelecting Features\nBelow, I use a function from the scikit-learn website. The function, SelectKBest, selects the k best features and removes the rest. I started by using k = 3, as 3 is the number of features we are supposed to use to train our model, however I didn’t get any qualitative features with k = 3. Because of this, I decided to make k larger, and decided to use k = 5. From here, I printed out the 5 best features and saw that the qualitative feature, island was one of the 5 best. For that reason, I decided to use island and my qualitative feature. Next, I had to decide what two quantitative features to use. From my tables above, I decided to start with culmen length and culmen depth due to the unique way to classify the penguins when both features are combined. Additionally, culmen length and culmen depth were both in the k top features.\n\n>>> from sklearn.feature_selection import SelectKBest\n>>> from sklearn.feature_selection import f_classif\n\nmodel= SelectKBest(f_classif, k=5).fit(X_train,y_train)\n\nX_Feature_Names=X_train.columns[model.get_support()]\n\n/Users/ceceziegler/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [9] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/ceceziegler/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n\nX_Feature_Names\n\nIndex(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Island_Biscoe'],\n      dtype='object')\n\n\n\n\nFitting my model\nI attempted to use many different models from the scikit-learn page, but after fitting them and cross validating, I determined that the LogisticRegression model was the best fit. Below is my models score the first time it was fit, and below that are the scores from cross validating the model 5 times.\n\nfrom sklearn.linear_model import LogisticRegression\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(LR, X_train[cols], y_train, cv=5)\ncv_scores\n\narray([0.98076923, 1.        , 0.98039216, 1.        , 1.        ])\n\n\n\n\n\nTesting The Model!\nBelow, I test and cross validate my model on the testing data. The score is printed for both the initial fitting and the cross validating. For all but one score in the cross validating, the model returned a score of 1. This shows that our model was well fitted.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(LR, X_test[cols], y_test, cv=5)\ncv_scores\n\narray([1.        , 1.        , 1.        , 1.        , 0.92307692])\n\n\n\nPlotting Decision Regions\nhttps://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#plotting-decision-regions Below, I plot the decision regions for the testing data. The graphs are broken up by the qualitative data, which for me is islands. I used code from the link above. The colors indicate each species, and we can see from the decsion regions that our model sucessfully classified our penguins.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\nFinal Visual!\nBelow is a jointplot graph that shows both a scatter plot and bell curves of the distribution for each species regarding culmen length and culmen depth which were the two quantitative features I focused on in my model. We can see from the graphs how the penguin species are very well separated into their own groups based on culmen length and depths which indicates that these were good features to select to train our model on to properly classify penguin species.\n\nsns.jointplot(data=train, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n<seaborn.axisgrid.JointGrid at 0x7fc890f1ea30>"
  }
]