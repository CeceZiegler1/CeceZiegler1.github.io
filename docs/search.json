[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Notebook to practice using linear regression model and RFE function built for final project\n\n\n\n\n\n\nMay 14, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nCSCI 0451 Final Project: Create a machine learning model that predicts a players bat speed utilizing Recursive Feature Elimination, Linear Regression and Random Forest Regression models.\n\n\n\n\n\n\nMay 14, 2023\n\n\nCece Ziegler, David Byrne, Julia Fairbank, Sam Ehrsam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the kernel logistic regression.\n\n\n\n\n\n\nMay 7, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of singular value decomposition of an image using unsupervised learning.\n\n\n\n\n\n\nApr 16, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of singular value decomposition of an image using unsupervised learning.\n\n\n\n\n\n\nApr 6, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of singular value decomposition of an image using unsupervised learning.\n\n\n\n\n\n\nApr 6, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the linear regression, using the analytical formula and gradient decent.\n\n\n\n\n\n\nMar 12, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the linear regression, using the analytical formula and gradient decent.\n\n\n\n\n\n\nMar 12, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of penguin classification.\n\n\n\n\n\n\nMar 7, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the logistic regression, using gradient decent.\n\n\n\n\n\n\nMar 2, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post illustrates my implementation of the perceptron algorithm, along with test cases to show its performance.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nCece Ziegler\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html",
    "href": "posts/perceptron_notebook/PerceptronBlog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Here is a link to my source code.\nhttps://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/perceptron_notebook/perceptron.py"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#graph-of-change-in-accuracy-with-each-iteration",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#graph-of-change-in-accuracy-with-each-iteration",
    "title": "Perceptron Blog",
    "section": "Graph of Change in Accuracy with each Iteration",
    "text": "Graph of Change in Accuracy with each Iteration\nThis graph shows the change in accuracy of the algorithm with each iteration. We can see here that it didn’t take the algorithm very long to converge to zero and reach an accuracy of 100%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "title": "Perceptron Blog",
    "section": "The graph below shows the accuracy over time of non linearly seperable data",
    "text": "The graph below shows the accuracy over time of non linearly seperable data\nWe can see from this graph that the accuracy fluctuates a lot with each iteration as the algorithm attempts to accuratley sort out the non-linearly separable data. It is unable to reach an accuracy of 100% with the best accuracy it reached coming at around 60%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/PerceptronBlog.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "href": "posts/perceptron_notebook/PerceptronBlog.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "title": "Perceptron Blog",
    "section": "Below is a graph of the change in accuracy over time for data with more than 5 features",
    "text": "Below is a graph of the change in accuracy over time for data with more than 5 features\nWe can see from this graph, similar to our 2D linearly separable data, the algorithm did not take a long time to converge to zero and reach an accuracy of 100%\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "CeceZieglerRepo/posts/example-blog-post/index.html",
    "href": "CeceZieglerRepo/posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "CeceZieglerRepo/posts/example-blog-post/index.html#math",
    "href": "CeceZieglerRepo/posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "CeceZieglerRepo/index.html",
    "href": "CeceZieglerRepo/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CeceZieglerRepo/about.html",
    "href": "CeceZieglerRepo/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "CeceZiegler1.github.io/posts/example-blog-post/index.html",
    "href": "CeceZiegler1.github.io/posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "CeceZiegler1.github.io/posts/example-blog-post/index.html#math",
    "href": "CeceZiegler1.github.io/posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "CeceZiegler1.github.io/index.html",
    "href": "CeceZiegler1.github.io/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CeceZiegler1.github.io/about.html",
    "href": "CeceZiegler1.github.io/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html",
    "href": "posts/perceptron_notebook/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Here is a link to my source code.\nhttps://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/perceptron_notebook/perceptron.py"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#graph-of-change-in-accuracy-with-each-iteration",
    "href": "posts/perceptron_notebook/index.html#graph-of-change-in-accuracy-with-each-iteration",
    "title": "Perceptron Blog",
    "section": "Graph of Change in Accuracy with each Iteration",
    "text": "Graph of Change in Accuracy with each Iteration\nThis graph shows the change in accuracy of the algorithm with each iteration. We can see here that it didn’t take the algorithm very long to converge to zero and reach an accuracy of 100%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "href": "posts/perceptron_notebook/index.html#the-graph-below-shows-the-accuracy-over-time-of-non-linearly-seperable-data",
    "title": "Perceptron Blog",
    "section": "The graph below shows the accuracy over time of non linearly seperable data",
    "text": "The graph below shows the accuracy over time of non linearly seperable data\nWe can see from this graph that the accuracy fluctuates a lot with each iteration as the algorithm attempts to accuratley sort out the non-linearly separable data. It is unable to reach an accuracy of 100% with the best accuracy it reached coming at around 60%\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron_notebook/index.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "href": "posts/perceptron_notebook/index.html#below-is-a-graph-of-the-change-in-accuracy-over-time-for-data-with-more-than-5-features",
    "title": "Perceptron Blog",
    "section": "Below is a graph of the change in accuracy over time for data with more than 5 features",
    "text": "Below is a graph of the change in accuracy over time for data with more than 5 features\nWe can see from this graph, similar to our 2D linearly separable data, the algorithm did not take a long time to converge to zero and reach an accuracy of 100%\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\n#np.random.seed(12345)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n\ny = 2*y -1\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n0.0"
  },
  {
    "objectID": "posts/gradient_blog/LogRegBlog.html",
    "href": "posts/gradient_blog/LogRegBlog.html",
    "title": "Gradient Decent",
    "section": "",
    "text": "Gradient Decent Implementation\nBelow, I show an example of my implementation of logistic regression using gradient decent. I make data and create a plot to show where my line separated the data. I then print out the loss and the accuracy to show how well the model performed. On average, my model averaged a loss between 0.15-0.19 which makes sense as the data is not linearly seperable, which makes this a reasonable loss.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nPrinting out Gradient Decent Loss\n\n#Loss\nprint(LR.loss)\n\n0.15683836141209298\n\n\n\n\n\nStochastic Gradient Implementation\nBelow is an example of a test of my stochastic gradient implementation. The stochastic gradient is similar to regular gradient decent, however it uses batch sizes to partiton the data and computes the gradient of the batch instead of the whole gradient at once. As we can see from the example below, depending on factors such as the size of the batch, stochastic gradient can help us minimize the loss. After running the regular gradient decent and the stochastic gradient decent many times each, the stochastic gradient decent averages a loss of about .13-.16 which is slightly lower than the loss of regular graident decent. Along with having on average a slightly better loss, the stochastic gradient method converges to a minimum faster than regular gradient decent\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, momentum = False, batch_size = 10)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LRS.loss)\n\n0.1364397783931314\n\n\n\n\nMomentum with Stochastic Gradient\nBelow is an example of a test using my stochastic gradient implementation including momentum. The model is the exact same as the stochastic gradient model, however one parameter is changed. When calling the fit method for regular stochastic gradient, I set the momentum parameter = to false. This indicated to set the coeficent beta to 0, meaning momentum won’t be added in when the weight vector is updated. However, when the momentum parameter is true, this indicates to include momentum, thus setting the beta coefficient to 0.8. Adding in momentum accelerates the process in which we are making our way down the gradient toward the minimum, thus helping to converge to the minimum faster. In this example, we also print out the loss which in this specific example was a solid loss at only 0.13, but the loss tends to remain in the same range as regular stochastic gradient\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLRS = LogisticRegression()\nLRS.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, momentum = True, batch_size = 10)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LRS.loss)\n\n0.1364397783931314\n\n\n\n\nLoss History Over Time\nBelow is a graph of each of the three models described above showing their loss history over each epoch/iteration. As we can see from the graph, regular gradient decent takes much longer to converge than stochastic gradient and stochastic gradient with momentum. Both the stochastic with momentum took 100 iteration, and stochastic without momentum took 200 iterations to converge whereas the regular gradient decent took 2100 iterations. This is a very significant difference in time and displays how stochastic gradient implementations can converge faster. We are also able to see from this how momentum helps speed up the time of convergence, as the model that included momentum took 100 less iteration to converge.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n100\n200\n2100\n\n\n\n\n\n\n\nAlpha too big to Converge\nBelow is an example in which the size of the alpha is too large which causes the model to be unable to converge to the minimum. In this test example, I set the alpha to 35 and the max_epochs to 100. Although the model still completed running, it’s loss was 0.26 which is much higher than we want and higher than any of the above model examples. This shows that the alpha size was too large, and the model was unable to converge to the proper minimum.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nfrom LogisticRegression import LogisticRegression\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 30, max_epochs = 100)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR.loss\n\n0.2608236468870702\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 2, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient small batch\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 20, \n                  alpha = .05) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient large batch\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n#This shows that small batch size takes longer to converge, 100 iterations for large batch compared to 600 for small batch\n\n100\n300\n\n\n\n\n\n\n\nExample showing batch size influencing convergence speed.\nIn the graph above, we can see how batch size affects the speed of convergence. Here, we show the convergence of two regular stochastic gradients, one with a batch size of 2, and the other with a batch size of 20. The function with a batch size of 20 took 100 iterations to converge, whereas the function with a batch size of 2 took 300 iterations to converge. This is three times the amount of iterations which shows that having a small batch size takes a longer time to converge.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 500, \n                  momentum = True, \n                  batch_size = 250, \n                  alpha = 0.1) \nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 500, \n                  momentum = False, \n                  batch_size = 250, \n                  alpha = 0.1)\nprint(len(LR.loss_history))\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n#Momentum converges faster when the batch size is large\n\n500\n1000\n\n\n\n\n\n\n\nExample: Momentum Converging Faster than Regular Stochastic:\nThe graph above shows an example of when momentum significantly speeds up convergence. In this example, we have a large batch size. With the large batch size, the stochastic momentum gradient only takes 500 iterations to converge, whereas the regular stochastic gradient takes 1000 iterations to converge. This is double the amount of iterations, showing that when the batch size is large, momentum helps the stochastic gradeient converge faster."
  },
  {
    "objectID": "posts/PenguinClassification/PenguinClassification.html",
    "href": "posts/PenguinClassification/PenguinClassification.html",
    "title": "Penguin Classification",
    "section": "",
    "text": "Explore\nI found helpful code and inspiration from the links below. https://seaborn.pydata.org/tutorial/introduction.html https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/classification-in-practice.html https://scikit-learn.org/stable/modules/feature_selection.html\n\nTable 1\nThe table below calculates the average flipper length and body mass for each species of penguin. From the table, we can see that Gentoo penguins appear to have the longest flippers and the greatest body mass. The Adelie and Chinstrap penguins are very close in both flipper length and body mass, with the chinstrip appeaer to be slighly larger in body mass for females where as Adelie are slightly larger in body mass for males. This indicates that aside from Gentoo penguins, it might be difficult to classify the penguins based on flipper length and body mass due to the similarity in numbers between two species of penguins.\n\ntrain.groupby(['Species', 'Sex'])[['Flipper Length (mm)', 'Body Mass (g)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n    \n    \n      \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      187.72\n      57\n      3337.28\n      57\n    \n    \n      MALE\n      192.69\n      55\n      4020.45\n      55\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      191.55\n      29\n      3514.66\n      29\n    \n    \n      MALE\n      199.67\n      27\n      3936.11\n      27\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      217.00\n      1\n      4875.00\n      1\n    \n    \n      FEMALE\n      212.93\n      42\n      4677.98\n      42\n    \n    \n      MALE\n      221.46\n      54\n      5502.31\n      54\n    \n  \n\n\n\n\n\n\nTable 2\nThe next table groups the penguins by island and counw how many of each species are at each island. We can see from the table that Adelie penguins occupy all three islands, whereas Chinstrap penguins only occupy the Dream island and Gentoo penguins only occupy the Biscoe island. The table indicates that island could be a good qualitative feature to use to classify the penguins.\n\ntrain.groupby(['Species', 'Island'])['Region'].aggregate([len]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      len\n    \n    \n      Species\n      Island\n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      Biscoe\n      35\n    \n    \n      Dream\n      41\n    \n    \n      Torgersen\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      Dream\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      Biscoe\n      101\n    \n  \n\n\n\n\n\n\nTable 3\nThe third table shows the average culmen length and culmen depth for each species of penguin broken up by gender. The table shows that Chinstrap and Gentoo penguins have similar culmen length whereas the Adelie penguins have much smaller culmen length. Culmen depth, on the other hand, is very similar between Adelie and Chinstrap penguins, and much smaller for Gentoo penguins. This indicates that together, culmen length and culmen depth can be good indicators of species as a penguin with longer culmen length and greater culmen depth would be classified as Chinstrap, a penguin with llng culmen length and small culmen depth would be classified as Gentoo, and a penguin with short culmen length and large culmen depth would be classified as Adelie.\n\ntrain.groupby(['Species', 'Sex'])[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      \n      \n      mean\n      len\n      mean\n      len\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      37.10\n      57\n      17.65\n      57\n    \n    \n      MALE\n      40.46\n      55\n      19.12\n      55\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      46.42\n      29\n      17.64\n      29\n    \n    \n      MALE\n      51.19\n      27\n      19.30\n      27\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      44.50\n      1\n      15.70\n      1\n    \n    \n      FEMALE\n      45.60\n      42\n      14.24\n      42\n    \n    \n      MALE\n      49.59\n      54\n      15.69\n      54\n    \n  \n\n\n\n\n\n\nGraph 1\nThe graph below plots the train data with culmen length on the x-axis, flipper length on the y-axis and is colored by penguin species. The graph then adds a line of best fit for each species. We can see from the graph that Gentoo penguins tend to be found in the middle to upper right quadrant of the graph, indcating greater flipper length and culmen length. Adelie penguins tend to be in the lower right quadrant of the graph indicating smaller culmen and flipper length. The chinstrap penguins tend to be in the middle of the graph showing smaller flipper length, but greater culmen length.\n\nimport seaborn as sns\n\nsns.lmplot(data=train, x=\"Culmen Length (mm)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fc88ebd5400>\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nSelecting Features\nBelow, I use a function from the scikit-learn website. The function, SelectKBest, selects the k best features and removes the rest. I started by using k = 3, as 3 is the number of features we are supposed to use to train our model, however I didn’t get any qualitative features with k = 3. Because of this, I decided to make k larger, and decided to use k = 5. From here, I printed out the 5 best features and saw that the qualitative feature, island was one of the 5 best. For that reason, I decided to use island and my qualitative feature. Next, I had to decide what two quantitative features to use. From my tables above, I decided to start with culmen length and culmen depth due to the unique way to classify the penguins when both features are combined. Additionally, culmen length and culmen depth were both in the k top features.\n\n>>> from sklearn.feature_selection import SelectKBest\n>>> from sklearn.feature_selection import f_classif\n\nmodel= SelectKBest(f_classif, k=5).fit(X_train,y_train)\n\nX_Feature_Names=X_train.columns[model.get_support()]\n\n/Users/ceceziegler/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [9] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/ceceziegler/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n\nX_Feature_Names\n\nIndex(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Island_Biscoe'],\n      dtype='object')\n\n\n\n\nFitting my model\nI attempted to use many different models from the scikit-learn page, but after fitting them and cross validating, I determined that the LogisticRegression model was the best fit. Below is my models score the first time it was fit, and below that are the scores from cross validating the model 5 times.\n\nfrom sklearn.linear_model import LogisticRegression\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(LR, X_train[cols], y_train, cv=5)\ncv_scores\n\narray([0.98076923, 1.        , 0.98039216, 1.        , 1.        ])\n\n\n\n\n\nTesting The Model!\nBelow, I test and cross validate my model on the testing data. The score is printed for both the initial fitting and the cross validating. For all but one score in the cross validating, the model returned a score of 1. This shows that our model was well fitted.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(LR, X_test[cols], y_test, cv=5)\ncv_scores\n\narray([1.        , 1.        , 1.        , 1.        , 0.92307692])\n\n\n\nPlotting Decision Regions\nhttps://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#plotting-decision-regions Below, I plot the decision regions for the testing data. The graphs are broken up by the qualitative data, which for me is islands. I used code from the link above. The colors indicate each species, and we can see from the decsion regions that our model sucessfully classified our penguins.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\nFinal Visual!\nBelow is a jointplot graph that shows both a scatter plot and bell curves of the distribution for each species regarding culmen length and culmen depth which were the two quantitative features I focused on in my model. We can see from the graphs how the penguin species are very well separated into their own groups based on culmen length and depths which indicates that these were good features to select to train our model on to properly classify penguin species.\n\nsns.jointplot(data=train, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n<seaborn.axisgrid.JointGrid at 0x7fc890f1ea30>"
  },
  {
    "objectID": "posts/LinearRegressionBlog/LinearRegBlog.html",
    "href": "posts/LinearRegressionBlog/LinearRegBlog.html",
    "title": "Linear Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/LinearRegressionBlog/LinearRegBlog.html#gradient-descent-score-over-time",
    "href": "posts/LinearRegressionBlog/LinearRegBlog.html#gradient-descent-score-over-time",
    "title": "Linear Regression",
    "section": "Gradient Descent Score over Time",
    "text": "Gradient Descent Score over Time\nBelow is a graph of how the score increases overtime with the gradient decent implementation. As we can see from the graph, the score starts off low, but increases with each iteration as the weight vector moves down the gradient. My program runs for 100 iterations, but we can see the best score is reached around 20-25 iteration mark and reamains for the rest of the iterations.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/UnsupervisedImageBlog/UnsupervisedImage.html",
    "href": "posts/UnsupervisedImageBlog/UnsupervisedImage.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here is a link to my sorce code: https://github.com/CeceZiegler1/CeceZiegler1.github.io/blob/main/posts/UnsupervisedImageBlog/SVD_Image.py"
  },
  {
    "objectID": "posts/UnsupervisedImageBlog/UnsupervisedImage.html#converting-imported-image-to-grey-scale",
    "href": "posts/UnsupervisedImageBlog/UnsupervisedImage.html#converting-imported-image-to-grey-scale",
    "title": "Unsupervised Learning",
    "section": "Converting Imported Image to Grey Scale",
    "text": "Converting Imported Image to Grey Scale\nBelow, I take my image that I imported and use the function provided in the assingment to convert my imaage to grey scale which also converts it to a matrix of values that allows us to compute singluar value decomposition. After converting the image to grey scale, I display both the original and the black and white image.\n\nfrom matplotlib import pyplot as plt\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]"
  },
  {
    "objectID": "posts/TimnitGebru/TimnitGebru.html",
    "href": "posts/TimnitGebru/TimnitGebru.html",
    "title": "Learning From Timnit Gebru",
    "section": "",
    "text": "Introduction\nTimnit Gebru grew up in Ethipoia and moved to the United States when she was 15 to flee the Eritrean-Ethiopian war. After experiencing difficulties with being allowed to live in the United States, Gebru finally settled in Sommerville Massachusetts where she attended high school. Gebru is an incrediblly influential and important figure due to her work regarding algorithmic bias in machine learning algorithms, specifically within computer vision. Through many experiences of racial bias Gebru has faced personally, she is passionate and knowelgeable on the topic. After working for world renowned tech companies including Apple and Google, Gebru has expressed the mistreatment and bias she has seen within these big tech companies. African Americans are mis-identified by computer vision technologies at a disproportionate rate. After writing a research paper for Google that detailed the potential negative effects of a new AI technology, a paper she was asked to write, Gebru left google as higher up athorities were not happy with what she wrote and didn’t want the paper published. After the poor experience with Google, Gebru doesn’t have much intrest in returning to work for a big tech company. Instead, she has started her own research intiative, Black in AI.\n\n\nDr. Gebru’s Talk\nIn the talk from Dr. Gebru, she addresses issues of fairness and ethics in technology, specifically computer vision. One of the main points Dr. Gebru brings up is how technology has done harm to black people and how black people, especially black women are under represented in the technology field. Although she mentions how technology as a whole has done harm to the black community, she speficially focusses on computer vision. Computer vision is a dangerous technology and we don’t know what it is being used for. Gebru brings up many applications that use computer vision technology in unethical way. For example, one app, Faception, profilies people based on their facial image. Other companies use facial deetction softwares during interviews in an attempt to track candidates non verbal responses and judge their performance in the interview based on what the facial dectection software finds. Not only are many facial recognition technologies unethical, but margonalized people are critisized and deprived disproportionatley. Gebru mentioned a few studies that showed that displayed this behavior. First she mentioned how police in Baltimore are unethically obtaining photographs of Black protestors and using those images to arrest them for unwarranted reasons. Next she describes how black women specifically are disproportionatley mis-identified by facial recognition technologies from major tech companies such as Google and Microsoft. Through these examples, Gebru identifies how it is not just the algorithms themselves that are bias, but there is bias in the training data. It is not an AI specific problem, but a problem of who is seen and who is heard. The bottom line is the majority of data collected is on white males. She describes how in an attempt to obtain more diverse datasets, companies unethically obtain pictures and images of people. It is difficult to obtain this data in an ethical way which is where a big issue comes into play with AI and machine learning algorthims being biased due to non-diverse datasets.\ntd;lr: Computer vision and other AI technologies have bias due to a lack of diversity in training data, and they unfairly target people who are the most margonalized and vulnerable.\n\n\nQuestion\nWhat do you think is an effective and tangible step companies can take to address issues of inequality and bias in technology and machine learning programs?"
  },
  {
    "objectID": "posts/TimnitGebru.html",
    "href": "posts/TimnitGebru.html",
    "title": "Learning From Timnit Gebru",
    "section": "",
    "text": "Introduction\nTimnit Gebru grew up in Ethipoia and moved to the United States when she was 15 to flee the Eritrean-Ethiopian war. After experiencing difficulties with being allowed to live in the United States, Gebru finally settled in Sommerville Massachusetts where she attended high school. Gebru is an incrediblly influential and important figure due to her work regarding algorithmic bias in machine learning algorithms, specifically within computer vision. Through many experiences of racial bias Gebru has faced personally, she is passionate and knowelgeable on the topic. After working for world renowned tech companies including Apple and Google, Gebru has expressed the mistreatment and bias she has seen within these big tech companies. African Americans are mis-identified by computer vision technologies at a disproportionate rate. After writing a research paper for Google that detailed the potential negative effects of a new AI technology, a paper she was asked to write, Gebru left google as higher up athorities were not happy with what she wrote and didn’t want the paper published. After the poor experience with Google, Gebru doesn’t have much intrest in returning to work for a big tech company. Instead, she has started her own research intiative, Black in AI.\n\n\nDr. Gebru’s Talk\nIn the talk from Dr. Gebru, she addresses issues of fairness and ethics in technology, specifically computer vision. One of the main points Dr. Gebru brings up is how technology has done harm to black people and how black people, especially black women are under represented in the technology field. Although she mentions how technology as a whole has done harm to the black community, she speficially focusses on computer vision. Computer vision is a dangerous technology and we don’t know what it is being used for. Gebru brings up many applications that use computer vision technology in unethical way. For example, one app, Faception, profilies people based on their facial image. Other companies use facial deetction softwares during interviews in an attempt to track candidates non verbal responses and judge their performance in the interview based on what the facial dectection software finds. Not only are many facial recognition technologies unethical, but margonalized people are critisized and deprived disproportionatley. Gebru mentioned a few studies that showed that displayed this behavior. First she mentioned how police in Baltimore are unethically obtaining photographs of Black protestors and using those images to arrest them for unwarranted reasons. Next she describes how black women specifically are disproportionatley mis-identified by facial recognition technologies from major tech companies such as Google and Microsoft. Through these examples, Gebru identifies how it is not just the algorithms themselves that are bias, but there is bias in the training data. It is not an AI specific problem, but a problem of who is seen and who is heard. The bottom line is the majority of data collected is on white males. She describes how in an attempt to obtain more diverse datasets, companies unethically obtain pictures and images of people. It is difficult to obtain this data in an ethical way which is where a big issue comes into play with AI and machine learning algorthims being biased due to non-diverse datasets.\ntd;lr: Computer vision and other AI technologies have bias due to a lack of diversity in training data, and they unfairly target people who are the most margonalized and vulnerable.\n\n\nQuestion\nWhat do you think is an effective and tangible step companies can take to address issues of inequality and bias in technology and machine learning programs?"
  },
  {
    "objectID": "posts/KernelBlog/KernelLogBlog.html",
    "href": "posts/KernelBlog/KernelLogBlog.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Introduction\nIn this blog I will implement kernel logistic regression. Kernel logistic regression uses a kernel function to calculate the weights and is often used when there are too many data points for the regular regression model to be sucessful. Throughout this blog post, I will perform many experiments that show how my kernel logistic regression model performs regularily and while changing different aspects such as the gamma and noise. One note about my code is that on each experiment I run with my model I am getting a warning error that I have been unsucessful in figuring out how to get it to go away.\n\n\nCode Explanation\nThe goal of my code is to find the values of v that minimize the empicical loss. In the fit method of my code, I first pad x and then compute a kernel matrix using the kernel function as described in the init method. The kernel matrix encodes the similarity amongst data points in the trainin set. Next, after initializing a value for v, the method uses the scipy.optimize.minimize() function to minimize the empirical risk in terms of the weight vectors, v.\nIn my model, the empirical risk is the mean of the logistic loss function which measures the difference between the models predictions and the actual labels of the training data. Empirical loss is calculated uses the logistic loss function. First, it computes the predictions using the kernel matrix and weight vector, v. Next, it plugs the predictions into the logistic loss function to compare the predicted values with the true values of y. Finally, it takes the mean of the value obtained using logistic loss.\n\n\nInitial experiments\nBelow, I run two experiements with the code provided in the blog post assignment. If my kernel logistic regression model is running properly, it should produce an accuracy score at or above 90 percent. In both experiments, I create a set of random data to run my kernel logistic regression model on. With the first set of random data, the model produces an acurracy score of 98% and with the second set of random data, the model produced an accuracy score of 97.5%. This is a strong indication that my model is performming how it is supposed to, and it is ready to move onto other experiments where aspects of the inputs are altered.\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nscore = plt.gca().set_title(f\"Accuracy = {KLR.score(X, y)}\")\nprint(KLR.score(X, y))\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n0.98\n\n\n\n\n\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nscore = plt.gca().set_title(f\"Accuracy = {KLR.score(X, y)}\")\nprint(KLR.score(X, y))\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n0.975\n\n\n\n\n\n\n\nChoosing Gamma\nNext, I will experiment with changing the values of gamma. A very large gamma value allows for a perfect training accuracy, as it overfits the training data. However, the specificity to which it creates the regions for each data point with a high gamma value results in a low testing accuracy which is ulitmately more important than achieving a perfect training score. The first example below shows a situation with an extremely high gamma value that results in a perfect training score but would result in a poor valididation score. Next, I run an experiment varying the gamma value and creating a plot that displays the contrast between the training score and the validation score as the value of gamma increases. The experiment runs across the range from 10^-2 to 10^10. The graph shows that as gamma increases until 10^1, the validation score increases. However, once we get past a gamma value of 10, the validation score rapidly decreases while the training score increases to 100%. This shows that when implementing the kernel logistic regresion model, it is better to use a smaller value of gamma even if it means a smaller training score, as it will result in a greater validation score which is what we want.\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 1000000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n1.0\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#range from 10^-2 to 10^10\ngamma_range = np.logspace(-2, 10, num=7)\n\ntrain_arr = np.zeros_like(gamma_range)\ntest_arr = np.zeros_like(gamma_range)\n\n\nfor i, gamma in enumerate(gamma_range):\n\n    model = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n\n    model.fit(X_train, y_train)\n\n    train_arr[i] = model.score(X_train, y_train)\n    \n    test_arr[i] = model.score(X_test, y_test)\n    \n    \nplt.semilogx(gamma_range, train_arr, label='Training accuracy')\nplt.semilogx(gamma_range, test_arr, label='Testing accuracy')\nplt.xlabel('Gamma')\nplt.ylabel('Accuracy')\nplt.title('Kernel Logistic Regression Performance')\nplt.legend()\nplt.show()\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\n\n\n\n\nVarying Noise\nOur next experiments will involve varying the noise when creating our data set. Varying the noise affects how spread out our moon cresents are. Below I will perform two experiments. The first is with a small noise value of 0.5. In this experiment, The validation score decreases from the beginning with a slight increase at the 10^4th gamma value before decreasing agian and leveling out at about 0.5. The training score takes an intial dip at 10^0 unlike the experiment above, before increasing and again reaching a final training score of 100%. Next, I perform an experiment with a large noise value of 100. This experiment shows different results than each of the last two graphs. In this graph, the validation score takes an intial dip to 50% at 10^0 gamma value before going back up to 60% at 10^1 gamma value and staying at 60% for the rest of the experiment. The training accuracy reached 100% at a gamma value of 10 and stayed at 100% for the remainder of the experiment. From these two experiments, we see that a smaller noise is better from smaller values of gamma, but a greater noise is better for larger gamma values.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\n\nX, y = make_moons(200, shuffle = True, noise = 0.5)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#range from 10^-2 to 10^10\ngamma_range = np.logspace(-2, 10, num=7)\n\ntrain_arr = np.zeros_like(gamma_range)\ntest_arr = np.zeros_like(gamma_range)\n\n\nfor i, gamma in enumerate(gamma_range):\n\n    model = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n\n    model.fit(X_train, y_train)\n\n    train_arr[i] = model.score(X_train, y_train)\n    \n    test_arr[i] = model.score(X_test, y_test)\n    \n    \nplt.semilogx(gamma_range, train_arr, label='Training accuracy')\nplt.semilogx(gamma_range, test_arr, label='Testing accuracy')\nplt.xlabel('Gamma')\nplt.ylabel('Accuracy')\nplt.title('Kernel Logistic Regression Performance')\nplt.legend()\nplt.show()\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\n\nX, y = make_moons(200, shuffle = True, noise = 100)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#range from 10^-2 to 10^10\ngamma_range = np.logspace(-2, 10, num=7)\n\ntrain_arr = np.zeros_like(gamma_range)\ntest_arr = np.zeros_like(gamma_range)\n\n\nfor i, gamma in enumerate(gamma_range):\n\n    model = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n\n    model.fit(X_train, y_train)\n\n    train_arr[i] = model.score(X_train, y_train)\n    \n    test_arr[i] = model.score(X_test, y_test)\n    \n    \nplt.semilogx(gamma_range, train_arr, label='Training accuracy')\nplt.semilogx(gamma_range, test_arr, label='Testing accuracy')\nplt.xlabel('Gamma')\nplt.ylabel('Accuracy')\nplt.title('Kernel Logistic Regression Performance')\nplt.legend()\nplt.show()\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\n\n\n\n\nConcentric Circles\nBelow I will implement my kernel logistic regression model with data that forms concentric circles to see how it performs. It was difficult to find values of noise and gamma that would produce a model that learned well enough to perform well on testing data. What I found is that smaller values of both gamma and noise produced better training scores without overfitting the model.\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\n\nX, y = make_circles(n_samples=100, shuffle=True, noise=0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 0.5)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n0.84\n\n\n\n\n\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\n\nX, y = make_circles(n_samples=100, shuffle=True, noise=0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 2)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n0.88"
  },
  {
    "objectID": "posts/ML_Final_Proj/LinearRegressionRFE.html",
    "href": "posts/ML_Final_Proj/LinearRegressionRFE.html",
    "title": "Linear Regression and RFE",
    "section": "",
    "text": "Source code\nhttps://github.com/CeceZiegler1/ML_Final_Proj/blob/main/LinearRegressionAnalytic.py\nBelow, we fit our model on the x_train and and y_train datasets, and then print out the training and validation scores. This model is fitted on all 60 features in the dataset. We can see from the scores, that it is not performing great, as a validation score below 50% indicates we could do better by just randomly selecting. We are going to perform a recursive feature elimination that we also implemented in our source code to see if we can find the optimal number of features to use to obtain the best score.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\n\n#Seeing how the model performs without RFE\n\nLR = LinearRegressionAnalytic()\nLR.fit(x_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(x_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(x_test, y_test).round(4)}\")\n\nTraining score = 0.5258\nValidation score = 0.4605\n\n\nBelow, we create an array to store the score that is produced with each different number of features used in the model as selected by our RFE. We use a for loop to loop through each value from 1-60 and display the score at each iteration in a graph. What we see from the graph, is even with using fewer features, our score never gets above around 45%. The best scores come around 12-15 features and 55-60 feautres. Even still, the scores at these points aren’t very good. Although we were hoping linear regression would perform well on our dataset, it doesn’t appear to be the case. Because of this, we are going to implement a random forest tree to see if we can obtain a better validation score on our dataset.\n\n\n# compute the score for each value of k\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\n\n[33, 34, 3, 35, 37, 6, 39, 2, 43, 45, 14, 19, 23, 28]\n\n\nBelow, we will show the 13 most important features as obtained through our rfe.\n\ndata.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\n\nx_train = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train, y_train)\nlr.score(x_test, y_test)\n\n0.386607442390802"
  },
  {
    "objectID": "posts/ML_Final_Proj/RFE.html",
    "href": "posts/ML_Final_Proj/RFE.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nmodel = LinearRegression()\nfeat_select = RFE(model, n_features_to_select = 10, step = 1)\n\nWhen using Recursive Feature Elimination, we need to select a few variables. First, we need to select what model we want to use. Since we are using multiple linear regression, we select linear regression. We also need to select how many features we want to use, and how many we want to remove during each recursion. These we chose arbitrarily for this example to be 10 and 1 respectively.\nIf we then run our RFE, we find that it classified our features as True or False, where True refers to features it selected and False ones it did not.\n\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True, False, False, False, False,  True,  True,  True,\n       False,  True,  True, False,  True])\n\n\nFrom this we can see that it selected features :42, 43, 46, 47, 52, 53, 54, 56, 57 and 59. This means that the data set we would train our model on is the following\n\ndata.iloc[:,[42,43,46,47,52,53,54,56,57,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      1850.5878\n    \n  \n\n637 rows × 10 columns\n\n\n\nHowever, 10 was arbitrarily chosen, and may not be the best choice. If we run a series of RFE’s which select for 1 more feature than the last we can then plot these values to find our optimal choice for the number of features.\n\n  \nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\n# define data values\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Linear Regression\")\nplt.show() \n\n\n\n\nFrom this chart we can see that we will be able to use a model with 15 features without losing too much accuracy, so we will next use RFE to find out the variables we want to use.\n\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True])\n\n\nThese refer to the following data set:\n\ndata.iloc[:,[42,43,46,47,48,49,50,51,52,53,54,56,57,58,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfx\n      min_lfx\n      max_lfy\n      min_lfy\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfy\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      121.2052\n      -885.2729\n      130.9304\n      -414.4391\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      545.3695\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      111.2187\n      -767.0614\n      128.0167\n      -475.8343\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      603.8510\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      178.4852\n      -827.7215\n      161.8112\n      -437.5895\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      599.4007\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      170.5486\n      -903.5394\n      187.1682\n      -430.2591\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      617.4273\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      176.3782\n      -939.6424\n      177.3536\n      -420.4205\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      597.7741\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      69.2794\n      -693.0041\n      121.9773\n      -342.0296\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      464.0069\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      60.6210\n      -669.8240\n      129.1773\n      -342.2472\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      471.4245\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      56.2369\n      -670.0392\n      111.4454\n      -329.7390\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      441.1844\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      67.6610\n      -734.0559\n      149.3230\n      -383.7818\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      533.1048\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      91.0192\n      -695.4843\n      150.5726\n      -252.4603\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      403.0329\n      1850.5878\n    \n  \n\n637 rows × 15 columns\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\n\nHowever, linear regression is not the only model that can deal with continous data. We can also use random forest regression. If we use random forest regression, and select 15 features again, we get a higher score than we did with linear regression. We also get slightly different features selected.\n\nfeat_select.fit(x_train, y_train)\nprint(feat_select.score(x_test, y_test))\nfeat_select.support_\n\nHowever, we don’t know if 15 is the optimal amount of features. As such, we can follow the same route we did with linear regression and graph our scores.\n\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\nThis shows us that any number of features over 10 will give us a model as good as using more. Therefore, to keep things similar between our two models we can use 15 features."
  },
  {
    "objectID": "posts/ML_Final_Proj/RandomForestRegressorAnalysis.html",
    "href": "posts/ML_Final_Proj/RandomForestRegressorAnalysis.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nfrom RandomForestRegressor import RandomForest\n\n\nrf = RandomForest()\n\n\nrf.fit(x_train, y_train, 1000, 500)\n\nWe also chose to implement and train a second model on our data. We implemented Random Forest Regression, since it is a very powerful model for making predictions with continous data. When we train this model on our whole data set, we get a much better validation score than with linear regression. However, just like with linear regression we can use feature reduction to reduce overfitting.\n\nrf.score(x_test, y_test)\n\n0.5699951488283441\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\n\n\n\nWe can see that this machine doesn’t suffer as much from overfitting, and has a higher validation score than Linear Regression.\n\nx_train = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train, y_train, 1000,500)\nrf.score(x_test, y_test)\n\nUsing the features we found from our RFE we get this score."
  },
  {
    "objectID": "posts/ML_Final_Proj/index.html",
    "href": "posts/ML_Final_Proj/index.html",
    "title": "Predicting Bat Speed",
    "section": "",
    "text": "Source Code Here are the two links to the source code for our linear regression/RFE and random forest model. https://github.com/CeceZiegler1/ML_Final_Proj/blob/main/LinearRegressionAnalytic.py https://github.com/CeceZiegler1/ML_Final_Proj/blob/main/RandomForestRegressor.py"
  },
  {
    "objectID": "posts/ML_Final_Proj/index.html#personal-reflection",
    "href": "posts/ML_Final_Proj/index.html#personal-reflection",
    "title": "Predicting Bat Speed",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nI learned a lot in the process of completing our final project. First, I learned about recursive feature elimination and how it can be used in conjunction with different models to select the most important features. It was beneficial to learn how this function worked through the SKLearn package, and it was rewarding to build my own version of the function in our implementation of the linear regression model. I also learned how it can be difficult to find a model that works best for predicting on different data sets. We felt good about our different models going into the building and testing as we had researched what models worked well for predicting continuous numeric values. However, as we see from our results, our models weren’t super sucessful. I learned that it can be a long process that involves a lot of trial and error to build a model that performs well on your specific data set. I was hoping to learn more about which features are most important in predicting bat speed to potentially help me with my own swing, however due to the low accuracy scores and lack of coherence between the features each model pulled as being most important, I didn’t learn as much as I had hoped to in this sense.\nI am very satisfied with what I acheived during this project. My goal was to complete all of my tasks on time and help with the implementations and revisions of the project. I met this goal, as I finished all of my parts in a timely matter, and I built the linear regression model and RFE function we used with the linear regression implementation. I was an active participant in all of our group meetings and I helped with various written portions of the final report.\nI will carry what I learned from this project into various aspects of my life. First, I think it is important to be able to work in a group, especially in the technology world, and I learned skills regarding working on a project using git hub from this final. Aditionally, I enhanced my skills in building machine learning models and experimenting with data. I learned skills to determine which models are better suited for which data set, along with ways to pick out the best features. These are skills I foresee myself using in feature job opportunities and potential individual projects I have interest in working on. I am very excited about what I learned throughout this course and how I was able to apply and enhance those skills through this final project. Machine learning and data science is something I have a strong interest in pursuing as I move forward with my education and post grad life, and I am excited to apply the skills I learned throughout this class and during this project."
  }
]