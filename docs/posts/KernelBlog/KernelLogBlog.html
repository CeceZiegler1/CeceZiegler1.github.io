<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Cece Ziegler">
<meta name="dcterms.date" content="2023-05-07">
<meta name="description" content="This blog post illustrates my implementation of the linear regression, using the analytical formula and gradient decent.">

<title>My Awesome CSCI 0451 Blog - Kernel Logistic Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
    }
    </style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Kernel Logistic Regression</h1>
                  <div>
        <div class="description">
          This blog post illustrates my implementation of the linear regression, using the analytical formula and gradient decent.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Cece Ziegler </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 7, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In this blog I will implement kernel logistic regression. Kernel logistic regression uses a kernel function to calculate the weights and is often used when there are too many data points for the regular regression model to be sucessful. Throughout this blog post, I will perform many experiments that show how my kernel logistic regression model performs regularily and while changing different aspects such as the gamma and noise. One note about my code is that on each experiment I run with my model I am getting a warning error that I have been unsucessful in figuring out how to get it to go away.</p>
</section>
<section id="code-explanation" class="level1">
<h1>Code Explanation</h1>
<p>The goal of my code is to find the values of v that minimize the empicical loss. In the fit method of my code, I first pad x and then compute a kernel matrix using the kernel function as described in the init method. The kernel matrix encodes the similarity amongst data points in the trainin set. Next, after initializing a value for v, the method uses the scipy.optimize.minimize() function to minimize the empirical risk in terms of the weight vectors, v.</p>
<p>In my model, the empirical risk is the mean of the logistic loss function which measures the difference between the models predictions and the actual labels of the training data. Empirical loss is calculated uses the logistic loss function. First, it computes the predictions using the kernel matrix and weight vector, v. Next, it plugs the predictions into the logistic loss function to compare the predicted values with the true values of y. Finally, it takes the mean of the value obtained using logistic loss.</p>
</section>
<section id="initial-experiments" class="level1">
<h1>Initial experiments</h1>
<p>Below, I run two experiements with the code provided in the blog post assignment. If my kernel logistic regression model is running properly, it should produce an accuracy score at or above 90 percent. In both experiments, I create a set of random data to run my kernel logistic regression model on. With the first set of random data, the model produces an acurracy score of 98% and with the second set of random data, the model produced an accuracy score of 97.5%. This is a strong indication that my model is performming how it is supposed to, and it is ready to move onto other experiments where aspects of the inputs are altered.</p>
<div class="cell" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> KernelLogisticRegression <span class="im">import</span> KernelLogisticRegression <span class="co"># your source code</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> rbf_kernel</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_decision_regions</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="fl">.1</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> plt.gca().set_title(<span class="ss">f"Accuracy = </span><span class="sc">{</span>KLR<span class="sc">.</span>score(X, y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(KLR.score(X, y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.98</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-3-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> KernelLogisticRegression <span class="im">import</span> KernelLogisticRegression <span class="co"># your source code</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> rbf_kernel</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="fl">.1</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> plt.gca().set_title(<span class="ss">f"Accuracy = </span><span class="sc">{</span>KLR<span class="sc">.</span>score(X, y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(KLR.score(X, y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.975</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-4-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="choosing-gamma" class="level1">
<h1>Choosing Gamma</h1>
<p>Next, I will experiment with changing the values of gamma. A very large gamma value allows for a perfect training accuracy, as it overfits the training data. However, the specificity to which it creates the regions for each data point with a high gamma value results in a low testing accuracy which is ulitmately more important than achieving a perfect training score. The first example below shows a situation with an extremely high gamma value that results in a perfect training score but would result in a poor valididation score. Next, I run an experiment varying the gamma value and creating a plot that displays the contrast between the training score and the validation score as the value of gamma increases. The experiment runs across the range from 10^-2 to 10^10. The graph shows that as gamma increases until 10^1, the validation score increases. However, once we get past a gamma value of 10, the validation score rapidly decreases while the training score increases to 100%. This shows that when implementing the kernel logistic regresion model, it is better to use a smaller value of gamma even if it means a smaller training score, as it will result in a greater validation score which is what we want.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_decision_regions</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="dv">1000000</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(KLR.score(X, y))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> title <span class="op">=</span> plt.gca().<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"Accuracy = </span><span class="sc">{</span>KLR<span class="sc">.</span>score(X, y)<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>                      xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                      ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-5-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> rbf_kernel</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> KernelLogisticRegression <span class="im">import</span> KernelLogisticRegression <span class="co"># your source code</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">#range from 10^-2 to 10^10</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>gamma_range <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">10</span>, num<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>train_arr <span class="op">=</span> np.zeros_like(gamma_range)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>test_arr <span class="op">=</span> np.zeros_like(gamma_range)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, gamma <span class="kw">in</span> <span class="bu">enumerate</span>(gamma_range):</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma<span class="op">=</span>gamma)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    train_arr[i] <span class="op">=</span> model.score(X_train, y_train)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    test_arr[i] <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>plt.semilogx(gamma_range, train_arr, label<span class="op">=</span><span class="st">'Training accuracy'</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>plt.semilogx(gamma_range, test_arr, label<span class="op">=</span><span class="st">'Testing accuracy'</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Gamma'</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Kernel Logistic Regression Performance'</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-6-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="varying-noise" class="level1">
<h1>Varying Noise</h1>
<p>Our next experiments will involve varying the noise when creating our data set. Varying the noise affects how spread out our moon cresents are. Below I will perform two experiments. The first is with a small noise value of 0.5. In this experiment, The validation score decreases from the beginning with a slight increase at the 10^4th gamma value before decreasing agian and leveling out at about 0.5. The training score takes an intial dip at 10^0 unlike the experiment above, before increasing and again reaching a final training score of 100%. Next, I perform an experiment with a large noise value of 100. This experiment shows different results than each of the last two graphs. In this graph, the validation score takes an intial dip to 50% at 10^0 gamma value before going back up to 60% at 10^1 gamma value and staying at 60% for the rest of the experiment. The training accuracy reached 100% at a gamma value of 10 and stayed at 100% for the remainder of the experiment. From these two experiments, we see that a smaller noise is better from smaller values of gamma, but a greater noise is better for larger gamma values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> rbf_kernel</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> KernelLogisticRegression <span class="im">import</span> KernelLogisticRegression <span class="co"># your source code</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="fl">0.5</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">#range from 10^-2 to 10^10</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>gamma_range <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">10</span>, num<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>train_arr <span class="op">=</span> np.zeros_like(gamma_range)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>test_arr <span class="op">=</span> np.zeros_like(gamma_range)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, gamma <span class="kw">in</span> <span class="bu">enumerate</span>(gamma_range):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma<span class="op">=</span>gamma)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    train_arr[i] <span class="op">=</span> model.score(X_train, y_train)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    test_arr[i] <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>plt.semilogx(gamma_range, train_arr, label<span class="op">=</span><span class="st">'Training accuracy'</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>plt.semilogx(gamma_range, test_arr, label<span class="op">=</span><span class="st">'Testing accuracy'</span>)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Gamma'</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Kernel Logistic Regression Performance'</span>)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> rbf_kernel</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> KernelLogisticRegression <span class="im">import</span> KernelLogisticRegression <span class="co"># your source code</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">#range from 10^-2 to 10^10</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>gamma_range <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">10</span>, num<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>train_arr <span class="op">=</span> np.zeros_like(gamma_range)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>test_arr <span class="op">=</span> np.zeros_like(gamma_range)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, gamma <span class="kw">in</span> <span class="bu">enumerate</span>(gamma_range):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma<span class="op">=</span>gamma)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    train_arr[i] <span class="op">=</span> model.score(X_train, y_train)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    test_arr[i] <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>plt.semilogx(gamma_range, train_arr, label<span class="op">=</span><span class="st">'Training accuracy'</span>)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>plt.semilogx(gamma_range, test_arr, label<span class="op">=</span><span class="st">'Testing accuracy'</span>)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Gamma'</span>)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Kernel Logistic Regression Performance'</span>)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="concentric-circles" class="level1">
<h1>Concentric Circles</h1>
<p>Below I will implement my kernel logistic regression model with data that forms concentric circles to see how it performs. It was difficult to find values of noise and gamma that would produce a model that learned well enough to perform well on testing data. What I found is that smaller values of both gamma and noise produced better training scores without overfitting the model.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_decision_regions</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">100</span>, shuffle<span class="op">=</span><span class="va">True</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="fl">0.5</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(KLR.score(X, y))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> title <span class="op">=</span> plt.gca().<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"Accuracy = </span><span class="sc">{</span>KLR<span class="sc">.</span>score(X, y)<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>                      xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, </span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>                      ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.84</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-9-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_decision_regions</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">100</span>, shuffle<span class="op">=</span><span class="va">True</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(KLR.score(X, y))</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> title <span class="op">=</span> plt.gca().<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"Accuracy = </span><span class="sc">{</span>KLR<span class="sc">.</span>score(X, y)<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>                      xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, </span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>                      ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:33: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-z))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: divide by zero encountered in log
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))
/Users/ceceziegler/Desktop/CeceZiegler1.github.io/posts/KernelBlog/KernelLogisticRegression.py:36: RuntimeWarning: invalid value encountered in multiply
  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.88</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="KernelLogBlog_files/figure-html/cell-10-output-3.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>